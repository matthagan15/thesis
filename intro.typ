
#import "conf.typ": *

#heading("Introduction", level: 1, supplement: [Chapter]) <ch:intro>
A central goal of physics is to provide explanatory power for natural phenomenon observed in the laboratory. As we progress through the $21^"st"$ century, which at the time of writing is a quarter complete, the ability of our mathematics and current computers to provide succint and human understandable explanations is struggling with the sheer scale of complexity for many problems involving quantum mechanics. Simulating the electronic dynamics of a large molecule with around 30 or so electrons is intractable for even the largest exascale ($10^15$ FLOPS) supercomputers currently available. Even if a supercomputer to simulate 30 electrons could be built, simulating 31 electrons would require one twice as large.

This exponential increase in the difficulty of simulating quantum mechanical systems, exponential with respect to the number of particles, led Manin @manin1980 and Feynman @feynman2018simulating in the early 1980s to propose using computers based on quantum mechanics to simulate quantum systems. The first concrete algorithm for doing so was proposed in 1996 by Lloyd @lloyd1996universal. This application remains the most promising commercial applications of quantum computers after 3 decades of intense theoretic development. This thesis is concerned with algorithms that address two parts of simulating quantum system: the first is the decomposition of the time evolution operator $e^(i H t)$ into primitive operations that can be implemented on a quantum computer. The second is an algorithm to prepare thermal states of the form $e^(-beta H) / tr(e^(-beta H))$, which are frequently used whenever the low energy states of the system $H$ are studied. We will discuss these problems in more detail in @sec_intro_existing_algs.

Efforts to develop quantum algorithms with provable advantages over classical computers has let to a significant transfer of ideas between classical computer science and quantum physics. For example, the development of quantum machine learning algorithms @kerenidis2016quantum led to equivalently powerful classical algorithms @tang2019quantum and ultimately a better understanding of the input models used in recommender systems. The classical sum-of-squares optimization technique @steurer2021sosdegreereductionapplications was crucial in recent results on Hamiltonian learning theory @bakshi2023learningquantumhamiltonianstemperature, which was later followed up with a proof that entanglement of spin systems is exactly zero above a system dependent temperature @bakshi2024high. Methods from classical signal processing @harris1978signal @oppenheim1999discrete served as the inspiration behind the modern Quantum Signal Processing (QSP) routine @yoder2014fixed @low2016qsp @low2017optimal, a state-of-the-art technique for quantum simulation.

A strong argument could be made that Markov Chain Monte Carlo (MCMC) techniques are arguably the most important family of algorithms that stem from this exchange between fields. The problem that MCMC solves is to transform access from some kind of uniform randomness, such as a collection of random integers between 0 and 255, into a specified probability distribution $pi (x)$. This distribution is typically specified by a function $pi$ that can be efficiently computed for a given value of $x$ but is difficult to integrate over the entire state space. The first algorithm to do so, the Metropolis-Hastings algorithm @metropolis1953equation @betancourt2018conceptualintroductionhamiltonianmonte, was developed to sample from the Boltzmann distribution $pi(x) = e^(-beta H(x)) / (integral e^(-beta H(x) ) d x )$ of spin systems. Since then, the algorithm has proven foundational in fields such as machine learning @goodfellow2016deep, computational physics @heermann1988monte, and quantitative finance @musiela2006martingale. This interplay between statistical mechanics, computer science, and machine learning was recently recognized with Hopfield @hopfield1982neural and Hinton @lecun2015deep recieving the 2024 Nobel Prize in Physics.

The core idea behind the Metropolis-Hastings algorithm is to split the sampling of $pi(x)$ into two steps, which we will discuss now without going into technical details. A transition step in which the previous sample $x$ is taken and a new state $x'$ is generated. This transition function has to be able to explore the entire state space given enough time, a typical example could be a spherical Gaussian centered at $x$. The second step is to filter the proposed sample $x'$, if the sample is accepted then the state moves to $x'$ and if it is rejected the state of the sampler stays at $x$. The first filter developed, called the Metropolis filter, is given by $"Prob"["accept"] = min {1, pi(x') / (pi (x))}$. A more physically realistic and continuous filter can be given if the distribution $pi(x)$ is a Boltzmann distribution of a Hamiltonian $H(x)$. In this scenario the function $"Prob"["accept"] = 1 / (1 + e^(-beta (H(x') - H(x))))$ is known as the Glauber filter @glauber1963 and smoothly allows for transitions to higher energy (lower probability) states with an exponential decay in the energy difference. Either of these filters are sufficient for guaranteeing convergence to the distribution $pi (x)$, which is guaranteed by a condition known as detailed balance.

A variant of the Metropolis-Hastings algorithm routinely used in quantum condensed matter physics and in molecular dynamics is known as Path Integral Monte Carlo. This algorithm uses similar ideas to Trotter-Suzuki product formulas in which an operator exponential is decomposed into a "time-sliced" product $e^(-beta H) = e^(-beta sum_i H_i) = lim_(r -> oo) (e^(-beta / r H_1) ... e^(-beta / r H_L))^r$, where $e^(-beta H_i)$ is an imaginary time propagator. By inserting projections onto a known vector basis $sum_j ketbra(j, j)$ between each project, the transition amplitude can be written as a sum over all paths from $ket(i)$ to $ket(j)$
$
    bra(j) e^(-beta H) ket(i) = sum_(k_gamma) bra(j) lim_(r -> oo) (e^(-beta / r H_1) ketbra(k_1, k_1) e^(-beta / r H_2) ... e^(-beta / r H_(L-1)) ketbra(k_(L-1), k_(L-1)) e^(-beta / r H_L))^r ket(i).
$
A given path is a discrete choice $i -> k_(r (L-1)) -> k_(r(L-2)) -> .. -> k_1 -> j$ and is dependent on the discretization of the state space.

Standard techniques exist to bound the errors of these expressions @herman1982path based on the length of the paths sampled from. When implemented on a classical computer the number of samples needed to approximate this sum can grow exponentially. To generate these samples an initial path is drawn, either uniformly at random or from a heuristic. Then a new path is sampled via a transition function and filtered via a suitable filter function of the user's choice. The connection with the original Metropolis-Hastings algorithm is then clear, the state space in the original algorithm is upgraded to a path from $ket(i)$ to $ket(j)$ and transitions between samples corresponds to choosing new paths. One of the main insights in Pocrnic et al. @pocrnic2024composite is that new randomized algorithms for quantum compilation can be leveraged by classical computers to reduce the path length requirement of Path Integral Monte Carlo algorithms.

The most striking connection between computer science and classical thermodynamics is given in the Hamiltonian Monte Carlo (HMC) algorithm. This algorithm was formalized in Radford Neal's thesis @neal1993probabilistic at the University of Toronto on probabilistic inference. The algorithm varies from a traditional Metropolis-Hastings algorithm by incorporating a momentum information $p$ alongside the state space $x$. The target distribution $pi(x)$, which is assumed to come as a thermal distribution over a potential $V$ in the form $pi(x) prop e^(-beta V(x))$, is then lifted to a distribution over position and momentum as $pi(x, p) prop e^(-beta (p^2 / (2 m) + V(x)))$. Due to the commutativity of $x$ and $p$ classically we can factor this distribution as $e^(-beta (p^2 / (2 m) + V(x))) = e^(-beta p^2 / (2 m)) e^(-beta V (x))$ we see that the momemtum distribution is a straightforward Gaussian whose mean is 0 and standard deviation dictated by the inverse temperature $beta$ (typically $m = 1$ is taken).

Once we have changed the background structure of the sampling problem to include momentum, we can tweak the Metropolis-Hastings algorithm by changing the transition function to utilize this information. We start a round of the algorithm by sampling a random momentum from a Gaussian of mean 0 and width $1 \/ beta$. This momentum is then used to simulate the classical time dynamics governed by the Hamiltonian $H(x, p)$ for a given time $t$. This moves the state $(x, p)$ to a new state $(x', p')$. Since Hamiltonian dyanmics preserves energy, if we use the Glauber filter we can be assured that this new sample will be accepted with probability 1 in the limit of perfect numerical integration. This procedure can be shown to satisfy detailed balance as well, with some slight technical modifications, leading to a complete algorithm for sampling thermal distributions.

HMC provides a direct connection between sampling problems and thermalization. By providing a synthetic model of thermalization we get an algorithm for sampling abstract probability distributions, ones that may or may not occur in nature. At a high level, the synthetic model tells us that if the momentum of a particle can be repeatedly drawn from it's thermal distribution then the system dynamics will "convert" this to the same temperature of the position. Physically, we can apply this model to a thought experiment of a cold gas interacting with a hot plate. Any particular molecule could theoretically be tracked and it's momentum before and after contact with the plate could be known. However, as the number of particles approaches the thermodynamic limit this becomes impossible. We then just model the momentum after interaction with the plate as being drawn from the canonical distribution $e^(-beta p^2 / (2m ))$ dictated by the thermal vibrations of the hot plate.

The organizing theme for this thesis is to provide a quantum analog of Hamiltonian Monte Carlo. This would provide access to a quantum density matrix $rho(beta) = e^(-beta H) / tr(e^(-beta H))$, where $H$ is now a Hermitian operator on the Hilbert space $CC^(2^n)$, assuming $n$ 2-level systems or qubits. If one tries to promote position and momentum to operators and directly copy the HMC prescription the issue of noncommutativity of $x$ and $p$ means we cannot factor the density matrix into $e^(-beta p^2 / (2m)) dot e^(-beta V(x))$. One possible way to overcome this could be to reinterpret the momentum sampling with a randomly chosen momentum _shift_ via the operator $e^(i p_"shift" hat(x))$, where $p_"shift"$ is chosen from a Gaussian $e^(-beta p_"shift"^2)$ and we remind the reader that $x$ is an operator. This may seem like a reasonable idea, but by using path integral techniques it can be shown that interleaving these momentum shifts with Hamiltonian evolution as $EE_"shift" e^(i H t) e^(i p_"shift" x) ketbra(psi, psi) e^(-i p_"shift" x) e^(-i H t)$ actually leads to the distance to the thermal state _increasing_. Simple numerics for even a harmonic oscillator show that the state approaches the maximally mixed state $id / dim$, meaning the momentum shifts decohere the system state as opposed to thermalizing it.

To resolve this issue we look towards current proposed theories for quantum thermalization. There are currently three main ways physicists study how environments can interact with a quantum system. The most common approach studied in the subfield of open quantum systems is the Linbladian or Liovillian approach. In this framework a model for the environment is constructed and it's effects on the system are captured in the Liovillian operator. A standard model for the environment is that it is unboundedly large, retains no memory of interactions, and is coupled weakly to the system. A specific construction of this was studied by Davies @davies1974markovian and shown to lead to the thermalization of a wide range of systems. This model has recently led to a surge of quantum algorithms, which will be described in the following section.

The second main theory is known as the Eigenstate Thermalization Hypothesis (ETH). This theory is designed to study how a large _closed_ quantum system can have subsytems that appear thermal. In effect, the system acts as it's own bath. This contradicts our intuition of closed quantum systems, as we typically think of unitary dynamics as conserving energy and therefore unable to explore a canonical ensemble. The secret to ETH's success is that entanglement between the subsystems can cause small subsystems, for sufficiently "chaotic" Hamiltonians, to have reduced density matrices that appear thermal. Although ETH has shown to hold for quantum systems that resemble chaotic classical systems it remains to be proven for arbitrary Hamiltonians and the existence of counterexamples, known as many-body localization, is a heavily debated area.

This model served as the inspiration for developing our thermalization algorithm beyond the randomized momentum shifts as it is a closed system and therefore the dynamics are unitary. This makes it relatively easy to implement on a quantum computer. We initially began exploring how a collection of truncated harmonic oscillators could be coupled together to replicate ETH style thermalization. After this proved too difficult we moved to studying when just two harmonic oscillators could reach thermal equilibrium. To couple them together we decided to use a completely random interaction chosen from the Gaussian Unitary Ensemble (GUE), this is because we wanted to extend the oscillator to more general systems later. The crucial detail that made this work was that the gaps of the oscillators had to be close, and we ended up truncating one of the oscillators to only 2 energy levels.

This model of a single system interacting with a single spin had been previously studied under the name of the Repeated Interactions (RI) framework @ziman2002diluting @scarani2002thermalizing @ziman2005description. RI methods are very straightforward, they only involve simulating the system of interest and typically one extra spin that is periodically refreshed regimes. This allows RI to be analyzed in situations where the Linbladian approximations do not hold, such as non-Markovian environments or strong coupling regimes. This flexibility makes RI useful for applications such as thermometry @seah2019thermometry @shu2020thermometry, quantum batteries @seah2021battery @barra2022battery, and quantum thermal pumps @purkayastha2022periodically @bettmann2023thermodynamics @hewgill2020three @de2020quantum. Most algorithms for quantum computers that use a RI framework are used primarily for simulating Linbladian equations @pocrnic2024quantumsimulationlindbladiandynamics @di2024efficient, however the technique was developed seemingly independently by Shtanko and Movassagh for preparing thermal states of ETH Hamiltonians @shtanko2023preparingthermalstatesnoiseless.

Despite the advantages of the RI framework, one of the downsides is that a very clear model of interaction is needed between the system and environment. Most of the existing literature has focused on very specific systems, such as a single qubit or a 3-level system. This is a problem for developing quantum algorithms, as one typically wants an algorithm to work for as many inputs as possible. To resolve this issue we introduced a randomized interaction method, where the interaction term is drawn from a random ensemble of matrices. In order to avoid tuning the gaps of the environment to match the system we further showed that choosing gaps uniformly at random allows one to bypass this difficulty at the cost of an increased simulation time and more interactions are needed to cool. This allows us to overcome the main difficulty with RI models and show that RI techniques can be extended to arbitrary systems, making it a plausible theory for thermalizing processes in nature.

== Existing Quantum Algorithms <sec_intro_existing_algs>

=== Hamiltonian Simulation
Hamiltonian simulation is a `BQP`-Complete problem, meaning that any polynomial time quantum algorithm can be formulated as an instance of Hamiltonian simulation. It is one of the earliest quantum algorithms @lloyd1996universal and was the original inspiration for proposing quantum computers @feynman2018simulating. As such there has been a plethora of techniques for solving this problem and we cannot attempt a thorough review of the literature. We will instead mention at a high level the various techniques applicable to the results in this thesis.

To start, we will assume that the Hamiltonian $H$ is provided as a sum of Pauli operators $H = sum_(i = 1)^L h_i P_i$. This is not a theoretically restricted model, as all matrices can be written as a linear sum of Pauli operators, but it may not be a practical input model for all scenarios. For simulating fermions this can be a viable option due to mappings, such as the Jordan-Wigner and Bravyi-Kitaev encodings @Setia_2018, from fermionic operators to Pauli operators. There are two other main input models used, one in which $H$ is provided as a sparse matrix @aharonov2003adiabatic and the entries can be queried via an oracle and the other in which the matrix $H$ is encoded as a block of a unitary matrix on a larger Hilbert space @low2019hamiltonian. These input models are necessary for simulation algorithms with asymptotically optimal circuit depth @low2019hamiltonian but one downside is they require a logarithmic overhead in the number of qubits needed to perform the simulation. Pauli operators on the other hand can be can be exponentiated in the form $e^(i P theta)$ with no ancilla overhead, Figure 4.19 of Nielsen and Chuang @nielsen2002quantum, making product formulas memory optimal. Further, they do not take advantage of the commutator structure of the terms in the Hamiltonian, meaning Hamiltonians with a closed Lie algebraic structure can have provably better peformance @childs2021theory.



=== Thermal State Preparation

There are many existing quantum algorithms for preparing thermal states on quantum computers, this section will present those most related to our work. The first algorithm was developed by Poulin and Wocjan @poulin2009sampling and was based around using quantum phase estimation to work in the system's eigenbasis. By obtaining an entangled register of the eigenvector and eigenvalue $ket(psi_i)ket(E_i)$ one can then perform a Pauli $X$ rotation of an ancilla based on the contents of the energy register $E_i$. Using amplitude amplification on this ancilla then prepares the final state, which can be used to estimate thermal observables. This algorithm is robust and fairly straightforward to implement. The downsides are that it doubles the amount of qubits needed to represent the system and takes at least time $O(sqrt(dim / cal(Z)))$, which scales with the dimension in the ground state limit. This runtime is fairly rigid and there are not many avenues for special case analysis that may lead to improvements.

Efforts to adopt the Metropolis-Hastings algorithm, a workhorse routine for Monte Carlo techniques, began a little later. The first efforts by Temme @temme2011 work by straightforwardly implementing the Metropolis-Hastings algorithm, which we will not discuss here but can be found in @betancourt2018conceptualintroductionhamiltonianmonte, and solving the problem of rejecting samples via Marriot-Watrous rewinding @marriott2005quantum. This technique is rather delicate, and as a result the proof of correctness of this algorithm is typically seen as incomplete. Other methods have attempted to utilize techniques such as Szegedy walks @yung2012quantum but implicitly require infinite precision phase estimation, a computationally unrealistic task. Recently these effects have been mitigated @jiang2024quantummetropolissamplingweak, but the resulting algorithm still involves the use of phase estimation, a memory intensive operation.

As preparing thermal states is a `QMA`-Hard problem, we do not expect quantum algorithms to work efficiently for every possible system. The only way to avoid this general case difficulty is to focus on specific systems, commonly lattice systems as they are common in condensed matter and high energy physics. In @brandao_finite_2019 the decay of correlations in a lattice Hamiltonian is taken advantage of to provide thermal state preparation algorithms that scale sublinearly in the dimension of the Hilbert space. Similar ideas have been extended in the Quantum Imaginary Time Evolution (QITE) algorithm @motta2020determining, which approximates imaginary time evolution, or application of the operator $prop e^(- beta H)$, with local unitary operators. These algorithms fundamentally rely on notions of locality and therefore do not work for arbitrary systems.

Other techniques tend to be less inspired by phyics and more with traditional computer science techniques. For example, variational approaches @chowdhury2020variationalquantumalgorithmpreparing have strong inspirations from optimization algorithms. With the development of modern quantum algorithmic primitives, such as block-encodings, Quantum Signal Processing @low2017optimal, Linear Combinations of Unitaries (LCU) @childs2012hamiltonian, and Qauntum Singular Value Transformations (QSVT) @gilyen2019quantum, came along their application to thermal state preparation @chowdhury2016quantumalgorithmsgibbssampling @gilyen2019quantum. As the naive application of QSVT-like techniques to implement $e^(-beta H)$ tend to have circuit depths scaling as $O(e^(beta))$, due to amplitude amplification costs, these techniques are typically used as subroutines in other algorithms.

The last class of algorithms we discuss are inspired by dissipation with an environment. The most straightforward of these algorithms is the Dissipative Quantum Eigensolver @cubitt2023dissipativegroundstatepreparation, which prepares ground states via weak measurements. This algorithm is a stopped process, meaning it's runtime is a random variable and must be analyzed in expectation. This algorithm relies on weak measurements and as a result can be implemented with a single ancilla qubit, making it memory optimal. Extensions @zhang2023dissipative later showed how to tweak these weak measurements to prepare finite $beta$ Gibbs states.

The largest group of the dissipative algorithms are those based on simulating Linbladian operators @gilyen2024quantumgeneralizationsglaubermetropolis @zhan2025rapidquantumgroundstate @chen2023quantumthermalstatepreparation. The first of these algorithms was inspired by the Davies generators @davies1974markovian for open quantum sytems, which weakly couples a system to a infinite thermal bath. These algorithms construct a Linbladian that can be shown to satisfy fixed point conditions, typically the Kubo-Martin-Schwinger (KMS) conditions @kms1 @kms2, that lead to the thermal state of the system being fixed. The runtime of the algorithm then depends on bounding a quantity known as the "mixing time", which is a very active area of research @tong2025fastmixingweaklyinteracting @temme2013mixing @ramkumar2024mixingtimequantumgibbs. As Linbladians, also called Liouvillians due to the Liouvillian equation, are the primary technical tool for studying open quantum systems these algorithms have many decades of physical intuition to rely on when they will perform well. One last difficulty for quantum algorithms developers in utilizing these theories is to create unitaries that can be run on quantum computers that replicate the physical maps. There currently exist a variety of Linblad solvers @pocrnic2024quantumsimulationlindbladiandynamics @ding2024simulating @chen2023quantumthermalstatepreparation but each introduces a nontrivial overhead making an end-to-end analysis difficult.

The closest algorithm for thermal state preparation to those that we present in this thesis is a repeated interaction algorithm by Shtanko and Movassagh @shtanko2023preparingthermalstatesnoiseless. This algorithm works by preparing many qubits in a thermal state for randomly chosen gaps and then interacting these qubits via $k$-local Pauli strings. By simulating the time evolution of the system, along with these interactions, for a randomly chosen time and coupling constant, they are able to show rapid convergence to the thermal state for ETH satisfying Hamiltonians. Their algorithm for generic Hamiltonians does not work for all error tolerances and inverse temperatures $beta$. In @shtanko2023preparingthermalstatesnoiseless they mention that algorithms working for arbitrary low temperatures (high $beta$) and generic Hamiltonians was an open problem at their time of writing, which we have resolved.

== Organization of the thesis
This thesis aims to develop techniques that are capable of preparing quantum thermal states starting with only basic quantum computing primitives. In @ch:composite_simulations we show how arbitrary single qubit rotations and Controlled `NOT` gates can be utilized to implement a class of Hamiltonian simulations techniques known as product formulas. Hamiltonian simulation is a fundamental primitive in virtually all thermal state preparation algorithms, but particularly in ours. The product formulas we present in this chapter rely on the Hamiltonian being expressed as a sum of Pauli operators, which is always theoretically possible but may not be practical for all systems. This chapter introduces two existing product formulas, Trotter-Suzuki or just Trotter formulas which are deterministic and QDrift, which is a randomized product formula. We then extend this to a composite scenario, introduced in @hagan2023composite, where both Trotter and QDrift are used to build a partially randomized product formula. In @sec:composite_first_order we present the composition of a first order Trotter formula with a QDrift channel utilizing a first order decomposition into the two channels. In @sec:composite_higher_order we extend these results to higher order product formulas. We then present numerics in @sec_composite_numerics from the extension of these ideas to imaginary time evolution by Pocrnic et al. in @pocrnic2024composite. These empirical results confirm that the theoretic cost savings predicted theoretically can be realized in practice, even for small systems that can be simulated classically.

We then utilize these results in @ch:thermal_state_prep to realize our thermal state preparation algorithm. We first develop a weak-coupling expansion of our thermalizing channel in @sec_tsp_weak_coupling. One of the central tools we develop in this section is a reduction of our quantum dynamics to a classical Markov chain. This allows us to compute fixed points and determine how many applications of the quantum channel are necessary to reach a given trace distance $epsilon$ from the fixed point. In @sec_tsp_oscillator we apply these techniques to two specific systems: a two-level or qubit system and a more general truncated harmonic oscillator. We keep these two systems as separate as they tend to be of independent interest to researchers and our presentation avoids the more complicated environment gap averaging necessary for arbitrary systems. In @sec_tsp_generic_sys we provide thermalization results for arbitrary systems in two scenarios: one in which eigenvalue differences are known exactly and the other in which only a bound on the spectral norm $norm(H)$ is known.
