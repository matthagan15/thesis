
#import "conf.typ": *

#heading("Introduction", level: 1, supplement: [Chapter]) <ch:intro>
// A central goal of physics is to provide explanatory power for natural phenomenon observed in the laboratory. As we progress through the $21^"st"$ century the ability of our mathematics and current computers to provide succint human understandable explanations of quantum mechanical phenomenon is struggling with the sheer scale of complexity for many-body systems. Simulating the electronic dynamics of a large molecule with around 30 or so electrons is intractable for even the largest exascale ($10^15$ FLOPS) supercomputers currently available. Even if a classical computer cluster to simulate 30 electrons could be built, simulating 31 electrons would require one twice as large.

The simulation of many-body quantum systems is one of the most profound problems facing physicists in the $21^"st"$ century. Even for classical systems, the presence of chaotic orbits in the 3-body gravitational problem makes exact solutions intractable. For classical systems this problem is counteracted by the exponential increase in the scale of compute power via Moore's Law. The development of scalable algorithms allows us to take advantage of this increase in compute power, meaning simulations of complex systems such as the global climate or nuclear reactions can double in accuracy every few years. The presence of fundamental issues with quantum systems, such as the fermionic sign problem @signProblemOG, means that a doubling of compute power only translates to an ability to simulate a single additional particle. As a result, only systems with incredibly structured symmetries can be solved analytically and we are numerically restricted to very small systems.

This exponential increase in the difficulty of simulating quantum mechanical systems, exponential with respect to the number of particles, led Manin @manin1980 and Feynman @feynman2018simulating in the early 1980s to propose using computers based on quantum mechanics to simulate quantum systems. The first concrete algorithm for doing so was proposed in 1996 by Lloyd @lloyd1996universal. This application remains the most promising scientific and commercial use of quantum computers after 3 decades of intense theoretic development. This thesis is concerned with algorithms that address two parts of simulating quantum system: the first is the decomposition of the time evolution operator $e^(i H t)$ into primitive operations that can be implemented on a quantum computer. The second is an algorithm to prepare thermal states of the form $e^(-beta H) / tr(e^(-beta H))$, which are frequently used whenever the low energy states of the system $H$ are studied. Classically, these two problems are intrinsically linked in an algorithm known as Hamiltonian Monte Carlo (HMC). The motivation for this thesis was to develop a quantum mechanical analog of this algorithm. We will discuss these two problems, simulation and thermal state preparation of quantum systems, in more detail in @sec_intro_existing_algs.

Efforts to develop quantum algorithms with provable advantages over classical computers has let to a significant transfer of ideas between classical computer science and quantum physics. For example, the development of quantum machine learning algorithms @kerenidis2016quantum led to equivalently powerful classical algorithms @tang2019quantum and ultimately a better understanding of the input models used in recommender systems. The classical sum-of-squares optimization technique @steurer2021sosdegreereductionapplications was crucial in recent results on Hamiltonian learning theory @bakshi2023learningquantumhamiltonianstemperature, which was later followed up with a proof that entanglement of spin systems is exactly zero above a system dependent temperature @bakshi2024high. Methods from classical signal processing @harris1978signal @oppenheim1999discrete served as the inspiration behind the modern Quantum Signal Processing (QSP) routine @yoder2014fixed @low2017optimal, a state-of-the-art technique for quantum simulation.

A strong argument could be made that Markov Chain Monte Carlo (MCMC) techniques are arguably the most important family of algorithms that stem from this exchange between fields. The problem that MCMC solves is to transform access from some kind of uniform randomness, such as a collection of random integers between 0 and 255, into a specified probability distribution $pi (x)$. This distribution is typically specified by a function $pi$ that can be efficiently computed for a given value of $x$ but is difficult to integrate over the entire state space. The first algorithm to do so, the Metropolis-Hastings algorithm @metropolis1953equation @betancourt2018conceptualintroductionhamiltonianmonte, was developed to sample from the Boltzmann distribution $pi(x) = e^(-beta H(x)) / (integral e^(-beta H(x) ) d x )$ of spin systems. Since then, the algorithm has proven foundational in fields such as machine learning @goodfellow2016deep, computational physics @heermann1988monte, and quantitative finance @musiela2006martingale. This interplay between statistical mechanics, computer science, and machine learning was recently recognized with Hopfield @hopfield1982neural and Hinton @lecun2015deep recieving the 2024 Nobel Prize in Physics.

The core idea behind the Metropolis-Hastings algorithm is to split the sampling of $pi(x)$ into two steps, which we will discuss now without going into technical details. A transition step in which the previous sample $x$ is taken and a new state $x'$ is generated. This transition function has to be able to explore the entire state space given enough time, a typical example could be a spherical Gaussian centered at $x$. The second step is to filter the proposed sample $x'$, if the sample is accepted then the state moves to $x'$ and if it is rejected the state of the sampler stays at $x$. The first filter developed, called the Metropolis filter, is given by $"Prob"["accept"] = min {1, pi(x') / (pi (x))}$. A more physically realistic and continuous filter can be given if the distribution $pi(x)$ is a Boltzmann distribution of a Hamiltonian $H(x)$. In this scenario the function $"Prob"["accept"] = 1 / (1 + e^(-beta (H(x') - H(x))))$ is known as the Glauber filter @glauber1963 and smoothly allows for transitions to higher energy (lower probability) states with an exponential decay in the energy difference. Either of these filters are sufficient for guaranteeing convergence to the distribution $pi (x)$, which is guaranteed by a condition known as detailed balance.

A variant of the Metropolis-Hastings algorithm routinely used in quantum condensed matter physics and in molecular dynamics is known as Path Integral Monte Carlo. This algorithm uses similar ideas to Trotter-Suzuki product formulas in which an operator exponential is decomposed into a "time-sliced" product $e^(-beta H) = e^(-beta sum_i H_i) = lim_(r -> oo) (e^(-beta / r H_1) ... e^(-beta / r H_L))^r$, where $e^(-beta H_i)$ is an imaginary time propagator. By inserting projections onto a known vector basis $sum_j ketbra(j, j)$ between each project, the transition amplitude can be written as a sum over all paths from $ket(i)$ to $ket(j)$
$
    bra(j) e^(-beta H) ket(i) = sum_(k_gamma) bra(j) lim_(r -> oo) (e^(-beta / r H_1) ketbra(k_1, k_1) e^(-beta / r H_2) ... e^(-beta / r H_(L-1)) ketbra(k_(L-1), k_(L-1)) e^(-beta / r H_L))^r ket(i).
$
A given path is a discrete choice $i -> k_(r (L-1)) -> k_(r(L-2)) -> .. -> k_1 -> j$ and is dependent on the discretization of the state space.

Standard techniques exist to bound the errors of these expressions @herman1982path based on the length of the paths sampled from. When implemented on a classical computer the number of samples needed to approximate this sum can grow exponentially. To generate these samples an initial path is drawn, either uniformly at random or from a heuristic. Then a new path is sampled via a transition function and filtered via a suitable filter function of the user's choice. The connection with the original Metropolis-Hastings algorithm is then clear, the state space in the original algorithm is upgraded to a path from $ket(i)$ to $ket(j)$ and transitions between samples corresponds to choosing new paths. One of the main insights in Pocrnic et al. @pocrnic2024composite is that new randomized algorithms for quantum compilation can be leveraged by classical computers to reduce the path length requirement of Path Integral Monte Carlo algorithms.

The most striking connection between computer science and classical thermodynamics is given in the Hamiltonian Monte Carlo algorithm. This algorithm was formalized in Radford Neal's thesis @neal1993probabilistic at the University of Toronto on probabilistic inference. The algorithm varies from a traditional Metropolis-Hastings algorithm by incorporating a momentum information $p$ alongside the state space $x$. The target distribution $pi(x)$, which is assumed to come as a thermal distribution over a potential $V$ in the form $pi(x) prop e^(-beta V(x))$, is then lifted to a distribution over position and momentum as $pi(x, p) prop e^(-beta (p^2 / (2 m) + V(x)))$. Due to the commutativity of $x$ and $p$ classically we can factor this distribution as $e^(-beta (p^2 / (2 m) + V(x))) = e^(-beta p^2 / (2 m)) e^(-beta V (x))$ we see that the momemtum distribution is a straightforward Gaussian whose mean is 0 and standard deviation dictated by the inverse temperature $beta$ (typically $m = 1$ is taken).

Once we have changed the background structure of the sampling problem to include momentum, we can tweak the Metropolis-Hastings algorithm by changing the transition function to utilize this information. We start a round of the algorithm by sampling a random momentum from a Gaussian of mean 0 and width $1 \/ beta$. This momentum is then used to simulate the classical time dynamics governed by the Hamiltonian $H(x, p)$ for a given time $t$. This moves the state $(x, p)$ to a new state $(x', p')$. Since Hamiltonian dyanmics preserves energy, if we use the Glauber filter we can be assured that this new sample will be accepted with probability 1 in the limit of perfect numerical integration. This procedure can be shown to satisfy detailed balance as well, with some slight technical modifications, leading to a complete algorithm for sampling thermal distributions.

HMC provides a direct connection between sampling problems and thermalization. By providing a synthetic model of thermalization we get an algorithm for sampling abstract probability distributions, ones that may or may not occur in nature. At a high level, the synthetic model tells us that if the momentum of a particle can be repeatedly drawn from it's thermal distribution then the system dynamics will "convert" this to the same temperature of the position. Physically, we can apply this model to a thought experiment of a cold gas interacting with a hot plate. Any particular molecule could theoretically be tracked and it's momentum before and after contact with the plate could be known. However, as the number of particles approaches the thermodynamic limit this becomes impossible. We then just model the momentum after interaction with the plate as being drawn from the canonical distribution $e^(-beta p^2 / (2m ))$ dictated by the thermal vibrations of the hot plate.

The organizing theme for this thesis is to provide a quantum analog of Hamiltonian Monte Carlo. This would provide access to a quantum density matrix $rho(beta) = e^(-beta H) / tr(e^(-beta H))$, where $H$ is now a Hermitian operator on the Hilbert space $CC^(2^n)$, assuming $n$ 2-level systems or qubits. If one tries to promote position and momentum to operators and directly copy the HMC prescription the issue of noncommutativity of $x$ and $p$ means we cannot factor the density matrix into $e^(-beta p^2 / (2m)) dot e^(-beta V(x))$. One possible way to overcome this could be to reinterpret the momentum sampling with a randomly chosen momentum _shift_ via the operator $e^(i p_"shift" hat(x))$, where $p_"shift"$ is chosen from a Gaussian $e^(-beta p_"shift"^2)$ and we remind the reader that $x$ is an operator. This may seem like a reasonable idea, but by using path integral techniques it can be shown that interleaving these momentum shifts with Hamiltonian evolution as $EE_"shift" e^(i H t) e^(i p_"shift" x) ketbra(psi, psi) e^(-i p_"shift" x) e^(-i H t)$ actually leads to the distance to the thermal state _increasing_. Simple numerics for even a harmonic oscillator show that the state approaches the maximally mixed state $id / dim$, meaning the momentum shifts decohere the system state as opposed to thermalizing it.

To resolve this issue we look towards current proposed theories for quantum thermalization. There are currently three main ways physicists study how environments can interact with a quantum system. The most common approach studied in the subfield of open quantum systems is the Linbladian or Liovillian approach. In this framework a model for the environment is constructed and it's effects on the system are captured in the Liovillian operator. A standard model for the environment is that it is unboundedly large, retains no memory of interactions, and is coupled weakly to the system. A specific construction of this was studied by Davies @davies1974markovian and shown to lead to the thermalization of a wide range of systems. This model has recently led to a surge of quantum algorithms, which will be described in the following section.

The second main theory is known as the Eigenstate Thermalization Hypothesis (ETH) @srednicki1994chaos @deutsch_eigenstate_2018. This theory is designed to study how a large _closed_ quantum system can have subsytems that appear thermal. In effect, the system acts as it's own bath. This contradicts our intuition of closed quantum systems, as we typically think of unitary dynamics as conserving energy and therefore unable to explore a canonical ensemble. The secret to ETH's success is that entanglement between the subsystems can cause small subsystems, for sufficiently "chaotic" Hamiltonians @choi2023preparing, to have reduced density matrices that appear thermal. Although ETH has shown to hold for quantum systems that resemble chaotic classical systems it remains to be proven for arbitrary Hamiltonians and the existence of counterexamples, known as many-body localization @nandkishore_many-body_2015, is a heavily debated area.

The ETH model served as the inspiration for developing our thermalization algorithm beyond the randomized momentum shifts as it is a closed system and therefore the dynamics are unitary. This makes it relatively easy to implement on a quantum computer. We initially began exploring how a collection of truncated harmonic oscillators could be coupled together to replicate ETH style thermalization. After this proved too difficult we moved to studying when just two harmonic oscillators could reach thermal equilibrium. To couple them together we decided to use a completely random interaction chosen from the Gaussian Unitary Ensemble (GUE), this is because we wanted to extend the oscillator to more general systems later. The crucial detail that made this work was that the gaps of the oscillators had to be close, and we ended up truncating one of the oscillators to only 2 energy levels.

This model of a single system interacting with a single spin had been previously studied under the name of the Repeated Interactions (RI) framework @ziman2002diluting @scarani2002thermalizing @ziman2005description. RI methods are very straightforward, they only involve simulating the system of interest and typically one extra spin that is periodically refreshed regimes. This allows RI to be analyzed in situations where the Linbladian approximations do not hold, such as non-Markovian environments or strong coupling regimes. This flexibility makes RI useful for applications such as thermometry @seah2019thermometry @shu2020thermometry, quantum batteries @seah2021battery @barra2022battery, and quantum thermal pumps @purkayastha2022periodically @bettmann2023thermodynamics @hewgill2020three @de2020quantum. Most algorithms for quantum computers that use a RI framework are used primarily for simulating Linbladian equations @pocrnic2024quantumsimulationlindbladiandynamics @di2024efficient, however the technique was developed seemingly independently by Shtanko and Movassagh for preparing thermal states of ETH Hamiltonians @shtanko2023preparingthermalstatesnoiseless.

Despite the advantages of the RI framework, one of the downsides is that a very clear model of interaction is needed between the system and environment. Most of the existing literature has focused on very specific systems, such as a single qubit or a 3-level system. This is a problem for developing quantum algorithms, as one typically wants an algorithm to work for as many inputs as possible. To resolve this issue we introduced a randomized interaction method, where the interaction term is drawn from a random ensemble of matrices. In order to avoid tuning the gaps of the environment to match the system we further showed that choosing gaps uniformly at random allows one to bypass this difficulty at the cost of an increased simulation time and more interactions are needed to cool. This allows us to overcome the main difficulty with RI models and show that RI techniques can be extended to arbitrary systems, making it a plausible theory for thermalizing processes in nature.

== Existing Quantum Algorithms <sec_intro_existing_algs>

In this section we introduce the two main problems we will address in this thesis, time-independent Hamiltonian simulation and thermal state preparation. As these are two historically important problems in quantum computing, there is an incredibly large body of research on these topics. As such, we do not attempt to provide a completely thorough review of every existing technique or application. We will focus on the main algorithms discussed in the literature and present their relative advantages at a high level.

=== Hamiltonian Simulation

Hamiltonian simulation is a `BQP`-Complete problem, meaning that any polynomial time quantum algorithm can be formulated as an instance of Hamiltonian simulation. The problem is typically defined by assuming access to a primitive gate set for universal quantum computing, a standard is Clifford + T. We will assume access to arbitrary single qubit unitaries, which can be generated using arbitrary angle $Z$ rotations and Hadamards, along with controlled NOT gates. We then assume we are given an input of a Hamiltonian $H$, which can be given in one of 3 different input models, a real time parameter $t$, and a nonzero error threshold $epsilon$. Sometimes an initial state $ket(psi)$ can be specified but as Hamiltonian simulation is widely used as a subroutine we typically do not specify the input. We then would like to solve the Schrödinger equation
$
    diff_t ket(psi(t)) = i H(t) ket(psi(t)).
$
There are two different solutions depending if $H$ is time-dependent or time-independent, in this thesis we are concerned with the time-independent problem. In this scenario the solution is given by $ket(psi(t)) = U(t) ket(psi(0)) := e^(i H t) ket(psi(0))$. Our error threshold typically enters as a bound on the spectral norm of our implementation $tilde(U)$
$
    norm(tilde(U)(t) - U(t))_oo <= epsilon,
$
but we will also make use of a diamond distance bound on the overall channel $norm(tilde(U) (dot) tilde(U)^dagger - U (dot) U^dagger)_dmd <= epsilon$.

The earliest algorithm to solve this problem was given by Lloyd @lloyd1996universal in 1996. This method proposed using a class of algorithms known broadly today as product formulas, and his algorithm in particular as a Trotter-Suzuki or just Trotter formulas. The most common input model with product formulas is to express the Hamiltonian as a sum of Pauli operators $H = sum_(i = 1)^L h_i P_i$ which has a few advantages. First, it is a natural target for the compilation of fermionic systems due to the encoding of fermionic creation and annihilation operators into Paulis via the Jordan-Wigner or Bravyi-Kitaev encodings @jordan1993paulische @bravyi2002fermionic @Setia_2018. If an $n$ qubit Hamiltonian is known to be $k$-local, meaning each Pauli acts nontrivially on at most $k$ qubits, then we can upper bound the number of terms by $4^k binom(n, k) = O(4^k n^k)$, which is polynomial in $n$. Lastly, Pauli operators can be easily exponentiated, see Figure 4.19 of Nielsen of Chuang @nielsen2002quantum. The extra ancilla qubit used in Figure 4.19 can be eliminated fairly easily, making product formulas a memory optimal construction. This is in contrast to their time complexity, which in the worst case is suboptimal at $O(5^p t dot (t / epsilon)^(1 / (p)))$ for a $p^"th"$ order Trotter formula.

One of the main difficulties when assessing the viability of Trotter formulas is the difficulty of error analysis. For most of their existence it was observed that product formulas can have empirical error estimates that are orders of magnitude below naive bounds @childs2018toward. Recently, improvements in error analysis that includes the commutator structure of the Hamiltonian @childs2021theory have significantly lowered these analytic upper bounds, making product formulas state-of-the-art for certain lattice systems. It is a major research effort to further reduce the error bounds to improve the performance of product formulas, with efforts involving product formula correctors @bagherimehrab2024faster, entanglement analysis @zhao2024entanglement, randomization @campbell2019random @jin2021partially @nakaji2023qswift, partial randomization @hagan2023composite @pocrnic2024composite @gunther2025phase, fixed input subspaces @csahinouglu2021hamiltonian, concentration bounds @chen2021concentration, randomly permuting terms @childs2019faster, randomly sparsifying the Hamiltonian @sparsto, time-step bucketing @coalescing_con_wiebe, and time-step interpolation @rendon2024improved @watson2024exponentially @watson2024randomly. There are surely other techniques not included in this list.

There are two other main input models used, one in which $H$ is provided as a sparse matrix @aharonov2003adiabatic and the entries can be queried via an oracle and the other in which the matrix $H$ is encoded as a block of a unitary matrix on a larger Hilbert space @low2019hamiltonian. The sparse matrix model was crucial in initial walk based simulation algorithms @childs2009universal. However, this model was generalized to the Linear Combinations of Unitaries (LCU) model, in which $H = sum_(i = 1)^L h_i U_i$ and each $U_i$ is unitary. This clearly includes a Pauli decomposition but allows for more general unitaries. The LCU model is crucial to block-encode a Hamiltonian, the resource primitive used in QSP algorithms @yoder2014fixed @low2017optimal.

Block-encodings are used in a variety of quantum simulation algorithms, the most prominent being QSP @low2017optimal, Quantum Singular Value Transformations (QSVT) @gilyen2019quantum, and qubitization @low2019hamiltonian. Although these algorithms achieve asymptotically optimal depths of $O(lambda t + log(1 / epsilon) / (log log (1 / epsilon)))$, and QSP and qubitization can be implemented using only one extra qubit, the block-encoding constant $lambda$ can be quite large. Current efforts to reduce $lambda$ for quantum chemistry problems through techniques such as Tensor HyperContraction (THC) @tensorHypercontraction, Double Factorization (DF) @cohn2021quantum, and spectral amplification @rocca2024reducing @low2025fast are active areas of current simulation research.

The relative advantages of product formulas compared to modern QSP-like algorithms is an active area of research. Determining which simulation method is advantageous is highly dependent on the Hamiltonian structure and problem parameters. One of the main results in @ch:composite_simulations is that even comparing product formulas can be challenging, as different methods have lower resource requirements based on the time $t$ and error $epsilon$. Taking into account factors such as commutator structures, interaction picture transformations @low2018hamiltonian, entanglement, and so on can make the problem even more challenging. Exotic problems, such as high energy simulations, may require fundamentally new algorithms altogether, such as the path integral simulation method developed by Shum in @shum2024efficient. A main topic of @ch:composite_simulations is that splitting a Hamiltonian up into pieces that suit different Hamiltonian simulation algorithms may offer researchers new ways to take advantage of the wealth of techniques currently available.

=== Thermal State Preparation

Thermal state preparation in it's most rigorous form is to create a channel $Phi$ that can output an $epsilon$ approximation to the thermal state $rho(beta) = e^(-beta H) / (tr (e^(-beta H)))$. The Hamiltonian $H$ is assumed to be given in a computationally feasible form, such as one of the input models outlined above. Typically the $beta = 0$ (infinite temperature) input state or it's purification are used as starting states. The requirement on the output is given by
$
    norm(rho(beta) - Phi(id / dim))_1 <= epsilon.
$
It is an open question if relaxing this requirement, say to instead match thermal expectation values of 2-body correlators for a local Hamiltonain, can lead to more efficient algorithms.
This problem contains the ground state problem, as $lim_(beta -> oo) rho(beta) = ketbra(1, 1)$, if $ket(1)$ is the lowest energy eigenstate of $H$. Thermal state preparation for arbitrary $beta$ is therefore a computationally difficult problem, meaning the runtime scales in the worst case with the dimension of the Hilbert space, as the ground state problem is known to be `QMA`-Hard @kempe2005complexitylocalhamiltonianproblem.

The first algorithm to solve this problem on a quantum computer was developed by Poulin and Wocjan @poulin2009sampling and was based around using quantum phase estimation to work in the system's eigenbasis. By obtaining an entangled register of the eigenvector and eigenvalue $ket(psi_i)ket(E_i)$ one can then perform a Pauli $X$ rotation of an ancilla based on the contents of the energy register $E_i$. Using amplitude amplification on this ancilla then prepares the final state, which can be used to estimate thermal observables. This algorithm is robust and fairly straightforward to implement. The downsides are that it doubles the amount of qubits needed to represent the system and takes at least time $O(sqrt(dim / cal(Z)))$, which scales with the dimension in the ground state limit. This runtime is fairly rigid and there are not many avenues for special case analysis that may lead to improvements.

Efforts to adopt the Metropolis-Hastings algorithm, a workhorse routine for Monte Carlo techniques, began a little later. The first efforts by Temme @temme2011 work by straightforwardly implementing the Metropolis-Hastings algorithm, which we will not discuss here but can be found in @betancourt2018conceptualintroductionhamiltonianmonte, and solving the problem of rejecting samples via Marriot-Watrous rewinding @marriott2005quantum. This technique is rather delicate, and as a result the proof of correctness of this algorithm is typically seen as incomplete. Other methods have attempted to utilize techniques such as Szegedy walks @yung2012quantum but implicitly require infinite precision phase estimation, a computationally unrealistic task. Recently these effects have been mitigated @jiang2024quantummetropolissamplingweak, but the resulting algorithm still involves the use of phase estimation, a memory intensive operation.

As preparing thermal states is a `QMA`-Hard problem, we do not expect quantum algorithms to work efficiently for every possible system. The only way to avoid this general case difficulty is to focus on specific systems, commonly lattice systems as they are common in condensed matter and high energy physics. In @brandao_finite_2019 the decay of correlations in a lattice Hamiltonian is taken advantage of to provide thermal state preparation algorithms that scale sublinearly in the dimension of the Hilbert space. Similar ideas have been extended in the Quantum Imaginary Time Evolution (QITE) algorithm @motta2020determining, which approximates imaginary time evolution, or application of the operator $prop e^(- beta H)$, with local unitary operators. These algorithms fundamentally rely on notions of locality and therefore do not work for arbitrary systems.

Other techniques tend to be less inspired by phyics and more with traditional computer science techniques. For example, variational approaches @chowdhury2020variationalquantumalgorithmpreparing have strong inspirations from optimization algorithms. With the development of modern quantum algorithmic primitives, such as block-encodings, Quantum Signal Processing @low2017optimal, Linear Combinations of Unitaries (LCU) @childs2012hamiltonian, and Qauntum Singular Value Transformations (QSVT) @gilyen2019quantum, came along their application to thermal state preparation @chowdhury2016quantumalgorithmsgibbssampling @gilyen2019quantum. As the naive application of QSVT-like techniques to implement $e^(-beta H)$ tend to have circuit depths scaling as $O(e^(beta))$, due to amplitude amplification costs, these techniques are typically used as subroutines in other algorithms.

The last class of algorithms we discuss are inspired by dissipation with an environment. The most straightforward of these algorithms is the Dissipative Quantum Eigensolver @cubitt2023dissipativegroundstatepreparation, which prepares ground states via weak measurements. This algorithm is a stopped process, meaning it's runtime is a random variable and must be analyzed in expectation. This algorithm relies on weak measurements and as a result can be implemented with a single ancilla qubit, making it memory optimal. Extensions @zhang2023dissipative later showed how to tweak these weak measurements to prepare finite $beta$ Gibbs states.

The largest group of the dissipative algorithms are those based on simulating Linbladian operators @gilyen2024quantumgeneralizationsglaubermetropolis @zhan2025rapidquantumgroundstate @chen2023quantumthermalstatepreparation. The first of these algorithms was inspired by the Davies generators @davies1974markovian for open quantum sytems, which weakly couples a system to a infinite thermal bath. These algorithms construct a Linbladian that can be shown to satisfy fixed point conditions, typically the Kubo-Martin-Schwinger (KMS) conditions @kms1 @kms2, that lead to the thermal state of the system being fixed. The runtime of the algorithm then depends on bounding a quantity known as the "mixing time", which is a very active area of research @tong2025fastmixingweaklyinteracting @temme2013mixing @ramkumar2024mixingtimequantumgibbs. As Linbladians, also called Liouvillians due to the Liouvillian equation, are the primary technical tool for studying open quantum systems these algorithms have many decades of physical intuition to rely on when they will perform well. One last difficulty for quantum algorithms developers in utilizing these theories is to create unitaries that can be run on quantum computers that replicate the physical maps. There currently exist a variety of Linblad solvers @pocrnic2024quantumsimulationlindbladiandynamics @ding2024simulating @chen2023quantumthermalstatepreparation but each introduces a nontrivial overhead making an end-to-end analysis difficult.

The closest algorithm for thermal state preparation to those that we present in this thesis is a repeated interaction algorithm by Shtanko and Movassagh @shtanko2023preparingthermalstatesnoiseless. This algorithm works by preparing many qubits in a thermal state for randomly chosen gaps and then interacting these qubits via $k$-local Pauli strings. By simulating the time evolution of the system, along with these interactions, for a randomly chosen time and coupling constant, they are able to show rapid convergence to the thermal state for ETH satisfying Hamiltonians. Their algorithm for generic Hamiltonians does not work for all error tolerances and inverse temperatures $beta$. In @shtanko2023preparingthermalstatesnoiseless they mention that algorithms working for arbitrarily high $beta$ (low temperatures) and generic Hamiltonians was an open problem at their time of writing, which we have resolved.

== Organization of the thesis
In @ch:composite_simulations we show how arbitrary single qubit rotations and Controlled `NOT` gates can be utilized to implement a class of Hamiltonian simulations techniques known as product formulas. Hamiltonian simulation is a fundamental primitive in virtually all thermal state preparation algorithms, but particularly in ours. The product formulas we present in this chapter rely on the Hamiltonian being expressed as a sum of Pauli operators, which is always theoretically possible but may not be practical for all systems. This chapter introduces two existing product formulas, Trotter-Suzuki or just Trotter formulas which are deterministic and QDrift, which is a randomized product formula. We then extend this to a composite scenario, introduced in @hagan2023composite, where both Trotter and QDrift are used to build a partially randomized product formula. In @sec:composite_first_order we present the composition of a first order Trotter formula with a QDrift channel utilizing a first order decomposition into the two channels. In @sec:composite_higher_order we extend these results to higher order product formulas. We then present numerics in @sec_composite_numerics from the extension of these ideas to imaginary time evolution by Pocrnic et al. in @pocrnic2024composite. These empirical results confirm that the theoretic cost savings predicted theoretically can be realized in practice, even for small systems that can be simulated classically.

We then utilize these results in @ch:thermal_state_prep to realize our thermal state preparation algorithm. We first develop a weak-coupling expansion of our thermalizing channel in @sec_tsp_weak_coupling. One of the central tools we develop in this section is a reduction of our quantum dynamics to a classical Markov chain. This allows us to compute fixed points and determine how many applications of the quantum channel are necessary to reach a given trace distance $epsilon$ from the fixed point. In @sec_tsp_oscillator we apply these techniques to two specific systems: a two-level or qubit system and a more general truncated harmonic oscillator. We keep these two systems as separate as they tend to be of independent interest to researchers and our presentation avoids the more complicated environment gap averaging necessary for arbitrary systems. In @sec_tsp_generic_sys we provide thermalization results for arbitrary systems in two scenarios: one in which eigenvalue differences are known exactly and the other in which only a bound on the spectral norm $norm(H)$ is known.
