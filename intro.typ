#import "macros.typ": *
#import "conf.typ": *

#heading("Introduction", level: 1, supplement: "Chapter") <ch:intro>

The goal of this thesis is to serve as a blueprint for creating quantum channels that can prepare thermal states of arbitrary systems. This framework was primarily created as an algorithmic process to prepare input states of the form $rho (beta) = (e^(-beta H)) / tr (e^(-beta H))$, known as Gibbs, Boltzmann, or thermal states, for simulations on a digital, fault-tolerant quantum computer. As $rho (beta)$ approaches the ground state as the inverse temperature $beta$ diverges, meaning Gibbs states serve as useful proxies for problems in which ground states dictate features of dynamics, such as the electron wavefunction in banded condensed matter systems or in chemical scenarios. One of the key features of our algorithm is the addition of only one single extra qubit outside of those needed to store the state of quantum system being simulated. This may seem like a minor technical achievement, given the existence of Gibbs samplers that also have only one extra qubit or at worst a constant number of qubits overhead, but the way in which this single qubit is utilized in our algorithm highlights the connections between our channel as an algorithmic tool to prepare states of interest and the model of thermalization that we believe the physical world may actually follow. The rest of this introduction is to provide context for how the technical results contained in later chapters of this thesis contribute to the growing interplay between Physics and Theoretical Computer Science within the realm of Quantum Computing.

The easiest way to grasp the context of this thesis is to first understand classical thermal states. In classical mechanics we typically have access to a Hamiltonian $H$ that is a function on phase space $(x, p)$ to the reals $RR$. We can turn this function into a probability distribution via the canonical ensemble $p_beta (x, p) = (e^(- beta H(x, p))) / (integral d x d p e^(-beta H(x, p)))$, which can be thought of as the "density" of particles near a particular $(x,p)$ in phase space for a thermodynamically large collection of non-interacting particles under the same Hamiltonian $H$. The inverse temperature $beta$, typically taken to be $1 / (k_beta T)$ where $k_beta$ is Boltzmann's constant and T the temperature, plays an important role in shaping the distribution $p_beta(x, p)$. For example, in the $beta -> oo$ the distribution becomes concentrated at the minimum energy points of $H(x, p)$, which correspond to points with zero momentum $p=0$ and are minimums of the potential energy $V(x)$. This shows that the problem of preparing classical thermal states somehow ``contains'' the problem of function optimization, and in fact being able to sample from classical thermal states for arbitrarily large $beta$ allows us to approximate the partition function $cal(Z) = integral e^(-beta H(x,p)) d x d p$, which is known to be a \#P-Hard computational problem.

In order to understand when classical systems can reach thermal equilibrium we need to understand where does the concept of temperature come from? We can use the following thought experiment of two systems $A$ and $B$ that are completely isolated from their surroundings, each initially has some internal energy $U_A$ and $U_B$. We could define entropies $sigma_A$ and $sigma_B$ from the distribution of the energy of each particle, averaged over some time window, and declare that an equilibrium has been reached between the two systems when the total entropy is constant
$
    d sigma = d sigma_A + d sigma_B = 0.
$
Then we know that the total energy of the two systems must remain constant, due to thermal isolation, so $d U = d U_A + d U_B = 0$. However, in this scenario the energy of the system is only a function of the entropy, so we have
$
    d U = (diff U_A) / (diff sigma_A) d sigma_A + (diff U_B) / (diff sigma_B) d sigma_B = ((diff U_A) / (diff sigma_A) - (diff U_B) / (diff sigma_B) ) d sigma_A = 0.
$
This implies that when the two systems are in equilibrium $(diff U_A) / (diff sigma_A) - (diff U_B) / (diff sigma_B) = 0$ and the property $(diff U_A) / (diff sigma_A)$ is equal to that of $B$. This is a good notion of temperature, so we define $T = 1 \/ (diff U) / (diff sigma)$.

The rationale for introducing this experiment is two-fold. On the first, it gives us a very rigorous definition of temperature by isolating exactly the condition meant behind "thermal equilibrium". We find that we can still define temperature even if we are not sure of how the two systems exchange energy. All we really need to know is that the systems are completely isolated and that energy is exchanged _somehow_, and that this energy exchange is captured by the entropy. The second main point of this experiment is that it gives a very concrete way to thermalize a system to some temperature $T$: if you have another system at that temperature just put the two into contact! Eventually they will equilibrate to some intermediate temperature and the process can be repeated.

Although the above result tells us that two systems in thermal contact with each other will eventually reach thermal equilibrium it doesn't exactly tell us _how_ it does so. Empirically, we have found a few different ways in which two systems can exchange heat:
- They can trade photons at various wavelengths, described by the Plank law, which then get "absorbed" by the various parties. This is how the earth reaches a thermal equilibrium with the sun and the vacuum of space.
- The nuclei of molecules can vibrate, which can then cause nearby nuclei and electrons to oscillate as well through Coulomb's law. This is the process of conduction, and can be extended to rigid crystals via phonon theory.
- Convection allows for hot parts of a liquid or gas to mix with colder parts. This brings the two extremal parts of a system in closer contact and allows for their temperatures to average out quicker.
These three mechanisms are typically sufficient for classical systems to equilibrate. Typically rates of heat exchange are measured empirically and the random microscopic effects are essentially "averaged out" at the macroscopic into a single coefficient of heat transfer.

If one has complete control over some system, such as a refrigerator, and can prepare it in a thermal state of temperature $T_"cold"$, we can then utilize this to cool down other systems via equilibration. Given our understanding that preparing thermal states is an incredibly challenging problem in the worst case, this raises the question: can we mimick this cooling process algorithmically on digital computers to sample from thermal distributions? If nature is able to cool systems down efficiently, we should be able to do so as well on computer simulations.

When trying to simulate a classical system we can typically compute the energy of the system $H(x,p)$ given a configuration of position and momenta. For example if we are trying to prepare the thermal state for a gas of non-interacting molecules with no background potential then the energy is just $sum_i p_i^2 \/ 2m$. If we encode the positions into standardized 64-bit floating point numbers, computing the energy can be done in order $O(N)$ time, where $N$ is the number of particles simulated. This is referred to as having oracle access to $H$, once we have a collection $(x, p)$ we can then compute $H$ as a function call. The challenging problem is to then take this oracle $H$ and output a list of samples ${s_1, s_2, ..., s_S}$, where $s_i$ is a pair of position and momenta $s_i = (x_i, p_i)$, that replicate the thermal statistics. We want to use these samples to compute difficult thermal quantities, given some observable $O(x,p)$ (e.g. the velocity of a particle $v = |arrow(p)|$) we would like the following to be as close to each other as possible
$
    1 / S sum_(i = 1)^S O(x_i, p_i) (e^(-beta H(x_i, p_i))) / (sum_(j = 1)^S e^(-beta H(x_j, p_j))) approx integral d x d p " " O(x, p) (e^(-beta H(x, p))) / (integral e^(-beta H(x', p')) d x' d p') .
$
This is known as Monte Carlo integration, as the integral on the right is replaced by the random process on the left.

The earliest technique for attacking such problems computationally was the Metropolis-Hastings algorithm. This generated samples ${s_i}$ using a two-step process that was repeated over and over. Starting with some random initial sample $s_0$ one then generates a proposed sample $tilde(s_1)$ using a random transition $T(x'| x)$ over the state space. For example, if the system of interest is a collection of spins $arrow(mu)_j$ then we can pick one spin uniformly and then randomly rotate it. Once we have the proposed new sample $tilde(s_1)$ we decide to accept it, in which case $s_1 = tilde(s_1)$ or reject it, in which case we go back to the previous point and set $s_1 = s_0$, with the randomized filter
$
    "Pr"[s_i = tilde(s_i)] = min(1, (e^(-beta H(tilde(s_i)) ) \/ cal(Z)) / (e^(-beta H(s_(i-1))) \/ cal(Z))) = min(1, e^(-beta (H(tilde(s_i)) - H(s_(i - 1))) )) .
$
This is known as the Metropolis filter and one key insight is that it only depends on the difference in energy between the previous state and the proposed state, the dependence on the partition function vanishes when taking the ratio. Since the initial sample can be random and the transition step is also randomized, this leads to an efficiently computable algorithm for generating new samples.

The last remaining question is if these samples are actually representative of the thermal distribution $p_beta (x,p)$. So long as the transition matrix is ergodic, meaning any starting state $(x', p')$ has a finite expected hitting time to reach any other arbitrary state $(x'', p'')$, then the thermal distribution $p_beta (x,p)$ satisfies a condition known as Detailed Balance which then guarantees convergence to $p_beta (x,p)$. As the above process is a Markov chain, since the distribution over the next sample only depends on the state of the current sample, we can define the Markov transition probability (which is different from the state transition probabilities) as $"Pr"[(x, p) -> (x', p')]$ and we say that a state probability distribution $pi$ satisfies Detailed Balance if the following equation holds
$
    "Pr"[(x_1,p_1) -> (x_2, p_2)] pi(x_1, p_1) = "Pr"[(x_2, p_2) -> (x_1, p_1)] pi(x_2, p_2).
$ <eq:og_detailed_balance>
This condition, along with ergodicity, is sufficient to prove that the probability $s_i$, for $i >> 1$, will be distributed according to $pi$. The amount of samples needed to converge to $pi$ is an incredibly difficult question to answer and the subject of much study on Markov chains.

One of the most recent improvements to this algorithm came about in the late 1980's and was formalized in Radford Neal's thesis in the Computer Science Department here at the University of Toronto @neal1993probabilistic. This new algorithm, now called Hamiltonian Monte Carlo (HMC) but previously known as Hybrid Monte Carlo, modified the existing Metropolis-Hastings algorithm by changing the state transition function. Instead of choosing the next proposed sample randomly, in HMC the momentum variable is chosen from a Gaussian distribution with variance proportional to $1 \/ beta$ and then the time dynamics generated by $H(x,p)$ are used to propose a new sample. Since time evolution does not change the energy, the Metropolis filter then accepts every sample in the limit of perfect numerical integration of Hamilton's equations of motion. This technique allows for much higher dimensional state spaces to be explored and empirically leads to less correlated samples.

We introduce HMC for the conceptual changes it makes to the original Metropolis-Hastings method. In Metropolis-Hastings the filter step is really what fixes the distribution to mimic the Gibbs distribution and the transition step is simply to guarantee ergodicity. The filtration step in this sense is rather artificial and more computational in spirit. In HMC the filtration step is virtually eliminated by instead utilizing the dynamics provided by nature. We are only able to take advantage of this because in classical mechanics the position and momentum variables "commute", meaning that the thermal distribution is really a product of distributions over position and momenta as
$
    e^(-beta H(x, p)) = e^(-beta p^2 / (2m) - beta V(x)) = e^(-beta p^2 / (2m)) dot e^(-beta V(x)),
$
and the partition function also splits as a product $cal(Z) = cal(Z)_x dot cal(Z)_p$. In a sense, HMC is using time dynamics to provide equilibration not between multiple particles but instead between momenta and position, and due to the simple nature of momentum thermal states as Gaussian random variables we can prepare these states at arbitrary temperatures relatively easy.

This crucial step of commutativity of $x$ and $p$ proves to be a difficult obstactle to overcome when extending HMC to a quantum mechanical setting. If one takes a na√Øve adoption to a continuous, single variable quantum Hamiltonian $H(hat(x), hat(p))$ and utilizes Gaussian distributed momentum _kicks_ $e^(i p_"kick" hat(x)) ket(psi)$, where $p_"kick"$ is Gaussian and $hat(x)$ is the position operator that generates momentum translations, one can then show that the maximally mixed state is the unique fixed point of the dynamics. This says that our model of a thermalizing environment as providing random momentum shifts is fundamentally *wrong*. Instead, one needs to come up with a simpler system that can be prepared in the thermal state at the desired temperature and use that to cool (or heat) the system to the target temperature.

The smaller environment that we ended up landing on is a single additional two level system, or qubit. The next difficulty is then to choose an interaction between the qubit and the system of interest that leads to thermalization. When looking to physics for inspiration, the situation for thermalization is much less clear in the quantum setting compared to the clear picture we have classically. Even determining a model for equilibrium is not as clear cut. If we look at a closed setting with two Hilbert spaces $cal(H)_A tp cal(H)_B$, similar to our classical scenario, one needs to determine the interaction model between the two halves. Further, the energy of each half is no longer a single function but rather a random variable dependent on the density matrix $rho$. Even computing derivatives of the mean energy is difficult as one needs an expression for $rho$ to compute the von Neumann entropy $- "Tr"[rho log rho]$.

There are two main approaches physicists have taken to avoid these complications: one involves a conjecture on large, closed quantum systems and the other involves modelling open quantum systems. The first involves a conjecture known as the Eigenstate Thermalization Hypothesis (ETH) @deutsch_eigenstate_2018 and outlines conditions in which a very large closed quantum system may replicate thermal expectations for _local_ observables on single particles. This framework has many connections to quantum chaos and has been proven to hold in such chaotic systems, but remains unproven for generic quantum systems, hence the "hypothesis" in the name. The second connection involves studying the effects of a quantum environment on a quantum system and essentially ignoring the state of the environment. By making several assumptions, such as an infinitely large environment, weak coupling, and an assumption known as Markovianity (where the system and environment remain in product states after each interaction), one can show that the thermal state is the unique fixed point for generic open systems. This is done using the Davies' Generators @davies1974markovian that give rise to a Linbladian evolution on the system with the desired fixed point. However, there exist many scenarios in nature where these assumptions are not met, such as finite sized environments that retain some correlations with the system and strong coupling situations. Finding Linblad evolutions that allow for generic thermalization for arbitrary system and environment pairs remains an open question. However, we do note that a flurry of quantum algorithms have been developed in recent years that are modeled off of these Linblad dynamics @chen2023quantumthermalstatepreparation @gilyen2024quantumgeneralizationsglaubermetropolis @ding2024efficientquantumgibbssamplers @cubitt2023dissipativegroundstatepreparation.

The model for thermalization that we propose is instead rooted in the concepts developed by Hamiltonian Monte Carlo: prepare an additional register in the desired thermal state, let time evolution mix the two constituents, and then refresh the controllable register. As mentioned before, we will make do with just a single qubit ancilla. In order for this process to work there are two difficulties that must be overcome in the quantum mechanical setting: the first is that a Hamiltonian must be chosen for the single ancilla, which amounts to choosing an energy gap we denote $gamma$, and the second is that an interaction term must be chosen between the ancilla and the system. One of the main contributions of this thesis is a rigorous proof that choosing from a suitably random ensemble of interactions is sufficient for thermalization.

This model turns out to have many beneficial properties that existing methods do not have. The first is that this model is very straightforward to implement on a quantum computer using existing algorithms. Evolution by a time independent Hamiltonian is the most complicated subroutine necessary. We will develop these primitives from scratch, showing how simple product formulas can be implemented at the gate level by utilizing a Pauli decomposition of the Hamiltonian. @ch:composite_simulations provides a new framework for developing new product formulas from existing ones. These ideas were developed in @hagan2022composite and allow for one to combine deterministic and randomized product formulas into a single channel. This makes it particularly well suited to simulating environmental effects on a system.

One other property that our model for thermalization has is that it explicitly models the environment, keeping track of it's non-equilibrium state throughout the evolution. This allows for us to turn the problem of preparing the system in a thermal state to instead allow for our ancilla qubit to probe and extract information about the system. For example, if we know that the system is already in a thermal state we can allow it the thermalize our ancilla qubit. By measuring the temperature of the ancilla we can infer the temperature of the system, allowing us to do thermometry without creating an interaction model. This is advantageous for complicated quantum systems, as there currently only exists thermometry techniques for harmonic oscillators.

The remainder of this thesis is organized as follows. In @ch:composite_simulations we introduce techniques for implementing existing product formulas on quantum computers, along with introducing new techniques for composing multiple simulation techniques for partitioned Hamiltonians. In @ch:thermal_state_prep we demonstrate how these techniques can be used to prepare thermal quantum states on fault-tolerant quantum computers. These two chapters represent the main technical contributions of this thesis and draw on the papers @hagan2022composite and @hagan2025thermodynamic for the analytic ideas and @pocrnic2024composite for numerical studies.
