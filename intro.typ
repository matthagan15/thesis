
#import "conf.typ": *

// #heading("Introduction", level: 1, supplement: "Chapter")

// The goal of this thesis is to serve as a blueprint for creating quantum channels that can prepare thermal states of arbitrary systems. This framework was primarily created as an algorithmic process to prepare input states of the form $rho (beta) = (e^(-beta H)) / tr(e^(-beta H))$, known as Gibbs, Boltzmann, or thermal states, for simulations on a digital, fault-tolerant quantum computer. As $rho (beta)$ approaches the ground state as the inverse temperature $beta$ diverges, meaning Gibbs states serve as useful proxies for problems in which ground states dictate features of dynamics, such as the electron wavefunction in banded condensed matter systems or in chemical scenarios. One of the key features of our algorithm is the addition of only one single extra qubit outside of those needed to store the state of quantum system being simulated. This may seem like a minor technical achievement, given the existence of Gibbs samplers that also have only one extra qubit or at worst a constant number of qubits overhead, but the way in which this single qubit is utilized in our algorithm highlights the connections between our channel as an algorithmic tool to prepare states of interest and the model of thermalization that we believe the physical world may actually follow. The rest of this introduction is to provide context for how the technical results contained in later chapters of this thesis contribute to the growing interplay between Physics and Theoretical Computer Science within the realm of Quantum Computing.

// The easiest way to grasp the context of this thesis is to first understand classical thermal states. In classical mechanics we typically have access to a Hamiltonian $H$ that is a function on phase space $(x, p)$ to the reals $RR$. We can turn this function into a probability distribution via the canonical ensemble $p_beta (x, p) = (e^(- beta H(x, p))) / (integral d x d p e^(-beta H(x, p)))$, which can be thought of as the "density" of particles near a particular $(x,p)$ in phase space for a thermodynamically large collection of non-interacting particles under the same Hamiltonian $H$. The inverse temperature $beta$, typically taken to be $1 / (k_beta T)$ where $k_beta$ is Boltzmann's constant and T the temperature, plays an important role in shaping the distribution $p_beta(x, p)$. For example, in the $beta -> oo$ the distribution becomes concentrated at the minimum energy points of $H(x, p)$, which correspond to points with zero momentum $p=0$ and are minimums of the potential energy $V(x)$. This shows that the problem of preparing classical thermal states somehow ``contains'' the problem of function optimization, and in fact being able to sample from classical thermal states for arbitrarily large $beta$ allows us to approximate the partition function $cal(Z) = integral e^(-beta H(x,p)) d x d p$, which is known to be a \#P-Hard computational problem.

// In order to understand when classical systems can reach thermal equilibrium we need to understand where does the concept of temperature come from? We can use the following thought experiment of two systems $A$ and $B$ that are completely isolated from their surroundings, each initially has some internal energy $U_A$ and $U_B$. We could define entropies $sigma_A$ and $sigma_B$ from the distribution of the energy of each particle, averaged over some time window, and declare that an equilibrium has been reached between the two systems when the total entropy is constant
// $
//     d sigma = d sigma_A + d sigma_B = 0.
// $
// Then we know that the total energy of the two systems must remain constant, due to thermal isolation, so $d U = d U_A + d U_B = 0$. However, in this scenario the energy of the system is only a function of the entropy, so we have
// $
//     d U = (diff U_A) / (diff sigma_A) d sigma_A + (diff U_B) / (diff sigma_B) d sigma_B = ((diff U_A) / (diff sigma_A) - (diff U_B) / (diff sigma_B) ) d sigma_A = 0.
// $
// This implies that when the two systems are in equilibrium $(diff U_A) / (diff sigma_A) - (diff U_B) / (diff sigma_B) = 0$ and the property $(diff U_A) / (diff sigma_A)$ is equal to that of $B$. This is a good notion of temperature, so we define $T = 1 \/ (diff U) / (diff sigma)$.

// The rationale for introducing this experiment is two-fold. On the first, it gives us a very rigorous definition of temperature by isolating exactly the condition meant behind "thermal equilibrium". We find that we can still define temperature even if we are not sure of how the two systems exchange energy. All we really need to know is that the systems are completely isolated and that energy is exchanged _somehow_, and that this energy exchange is captured by the entropy. The second main point of this experiment is that it gives a very concrete way to thermalize a system to some temperature $T$: if you have another system at that temperature just put the two into contact! Eventually they will equilibrate to some intermediate temperature and the process can be repeated.

// Although the above result tells us that two systems in thermal contact with each other will eventually reach thermal equilibrium it doesn't exactly tell us _how_ it does so. Empirically, we have found a few different ways in which two systems can exchange heat:
// - They can trade photons at various wavelengths, described by the Plank law, which then get "absorbed" by the various parties. This is how the earth reaches a thermal equilibrium with the sun and the vacuum of space.
// - The nuclei of molecules can vibrate, which can then cause nearby nuclei and electrons to oscillate as well through Coulomb's law. This is the process of conduction, and can be extended to rigid crystals via phonon theory.
// - Convection allows for hot parts of a liquid or gas to mix with colder parts. This brings the two extremal parts of a system in closer contact and allows for their temperatures to average out quicker.
// These three mechanisms are typically sufficient for classical systems to equilibrate. Typically rates of heat exchange are measured empirically and the random microscopic effects are essentially "averaged out" at the macroscopic into a single coefficient of heat transfer.

// If one has complete control over some system, such as a refrigerator, and can prepare it in a thermal state of temperature $T_"cold"$, we can then utilize this to cool down other systems via equilibration. Given our understanding that preparing thermal states is an incredibly challenging problem in the worst case, this raises the question: can we mimick this cooling process algorithmically on digital computers to sample from thermal distributions? If nature is able to cool systems down efficiently, we should be able to do so as well on computer simulations.

// When trying to simulate a classical system we can typically compute the energy of the system $H(x,p)$ given a configuration of position and momenta. For example if we are trying to prepare the thermal state for a gas of non-interacting molecules with no background potential then the energy is just $sum_i p_i^2 \/ 2m$. If we encode the positions into standardized 64-bit floating point numbers, computing the energy can be done in order $O(N)$ time, where $N$ is the number of particles simulated. This is referred to as having oracle access to $H$, once we have a collection $(x, p)$ we can then compute $H$ as a function call. The challenging problem is to then take this oracle $H$ and output a list of samples ${s_1, s_2, ..., s_S}$, where $s_i$ is a pair of position and momenta $s_i = (x_i, p_i)$, that replicate the thermal statistics. We want to use these samples to compute difficult thermal quantities, given some observable $O(x,p)$ (e.g. the velocity of a particle $v = |arrow(p)|$) we would like the following to be as close to each other as possible
// $
//     1 / S sum_(i = 1)^S O(x_i, p_i) (e^(-beta H(x_i, p_i))) / (sum_(j = 1)^S e^(-beta H(x_j, p_j))) approx integral d x d p " " O(x, p) (e^(-beta H(x, p))) / (integral e^(-beta H(x', p')) d x' d p') .
// $
// This is known as Monte Carlo integration, as the integral on the right is replaced by the random process on the left.

// The earliest technique for attacking such problems computationally was the Metropolis-Hastings algorithm. This generated samples ${s_i}$ using a two-step process that was repeated over and over. Starting with some random initial sample $s_0$ one then generates a proposed sample $tilde(s_1)$ using a random transition $T(x'| x)$ over the state space. For example, if the system of interest is a collection of spins $arrow(mu)_j$ then we can pick one spin uniformly and then randomly rotate it. Once we have the proposed new sample $tilde(s_1)$ we decide to accept it, in which case $s_1 = tilde(s_1)$ or reject it, in which case we go back to the previous point and set $s_1 = s_0$, with the randomized filter
// $
//     "Pr"[s_i = tilde(s_i)] = min(1, (e^(-beta H(tilde(s_i)) ) \/ cal(Z)) / (e^(-beta H(s_(i-1))) \/ cal(Z))) = min(1, e^(-beta (H(tilde(s_i)) - H(s_(i - 1))) )) .
// $
// This is known as the Metropolis filter and one key insight is that it only depends on the difference in energy between the previous state and the proposed state, the dependence on the partition function vanishes when taking the ratio. Since the initial sample can be random and the transition step is also randomized, this leads to an efficiently computable algorithm for generating new samples.

// The last remaining question is if these samples are actually representative of the thermal distribution $p_beta (x,p)$. So long as the transition matrix is ergodic, meaning any starting state $(x', p')$ has a finite expected hitting time to reach any other arbitrary state $(x'', p'')$, then the thermal distribution $p_beta (x,p)$ satisfies a condition known as Detailed Balance which then guarantees convergence to $p_beta (x,p)$. As the above process is a Markov chain, since the distribution over the next sample only depends on the state of the current sample, we can define the Markov transition probability (which is different from the state transition probabilities) as $"Pr"[(x, p) -> (x', p')]$ and we say that a state probability distribution $pi$ satisfies Detailed Balance if the following equation holds
// $
//     "Pr"[(x_1,p_1) -> (x_2, p_2)] pi(x_1, p_1) = "Pr"[(x_2, p_2) -> (x_1, p_1)] pi(x_2, p_2).
// $ <eq:og_detailed_balance>
// This condition, along with ergodicity, is sufficient to prove that the probability $s_i$, for $i >> 1$, will be distributed according to $pi$. The amount of samples needed to converge to $pi$ is an incredibly difficult question to answer and the subject of much study on Markov chains.

// One of the most recent improvements to this algorithm came about in the late 1980's and was formalized in Radford Neal's thesis in the Computer Science Department here at the University of Toronto @neal1993probabilistic. This new algorithm, now called Hamiltonian Monte Carlo (HMC) but previously known as Hybrid Monte Carlo, modified the existing Metropolis-Hastings algorithm by changing the state transition function. Instead of choosing the next proposed sample randomly, in HMC the momentum variable is chosen from a Gaussian distribution with variance proportional to $1 \/ beta$ and then the time dynamics generated by $H(x,p)$ are used to propose a new sample. Since time evolution does not change the energy, the Metropolis filter then accepts every sample in the limit of perfect numerical integration of Hamilton's equations of motion. This technique allows for much higher dimensional state spaces to be explored and empirically leads to less correlated samples.

// We introduce HMC for the conceptual changes it makes to the original Metropolis-Hastings method. In Metropolis-Hastings the filter step is really what fixes the distribution to mimic the Gibbs distribution and the transition step is simply to guarantee ergodicity. The filtration step in this sense is rather artificial and more computational in spirit. In HMC the filtration step is virtually eliminated by instead utilizing the dynamics provided by nature. We are only able to take advantage of this because in classical mechanics the position and momentum variables "commute", meaning that the thermal distribution is really a product of distributions over position and momenta as
// $
//     e^(-beta H(x, p)) = e^(-beta p^2 / (2m) - beta V(x)) = e^(-beta p^2 / (2m)) dot e^(-beta V(x)),
// $
// and the partition function also splits as a product $cal(Z) = cal(Z)_x dot cal(Z)_p$. In a sense, HMC is using time dynamics to provide equilibration not between multiple particles but instead between momenta and position, and due to the simple nature of momentum thermal states as Gaussian random variables we can prepare these states at arbitrary temperatures relatively easy.

// This crucial step of commutativity of $x$ and $p$ proves to be a difficult obstactle to overcome when extending HMC to a quantum mechanical setting. If one takes a naïve adoption to a continuous, single variable quantum Hamiltonian $H(hat(x), hat(p))$ and utilizes Gaussian distributed momentum _kicks_ $e^(i p_"kick" hat(x)) ket(psi)$, where $p_"kick"$ is Gaussian and $hat(x)$ is the position operator that generates momentum translations, one can then show that the maximally mixed state is the unique fixed point of the dynamics. This says that our model of a thermalizing environment as providing random momentum shifts is fundamentally *wrong*. Instead, one needs to come up with a simpler system that can be prepared in the thermal state at the desired temperature and use that to cool (or heat) the system to the target temperature.

// The smaller environment that we ended up landing on is a single additional two level system, or qubit. The next difficulty is then to choose an interaction between the qubit and the system of interest that leads to thermalization. When looking to physics for inspiration, the situation for thermalization is much less clear in the quantum setting compared to the clear picture we have classically. Even determining a model for equilibrium is not as clear cut. If we look at a closed setting with two Hilbert spaces $cal(H)_A tp cal(H)_B$, similar to our classical scenario, one needs to determine the interaction model between the two halves. Further, the energy of each half is no longer a single function but rather a random variable dependent on the density matrix $rho$. Even computing derivatives of the mean energy is difficult as one needs an expression for $rho$ to compute the von Neumann entropy $- "Tr"[rho log rho]$.

// There are two main approaches physicists have taken to avoid these complications: one involves a conjecture on large, closed quantum systems and the other involves modelling open quantum systems. The first involves a conjecture known as the Eigenstate Thermalization Hypothesis (ETH) @deutsch_eigenstate_2018 and outlines conditions in which a very large closed quantum system may replicate thermal expectations for _local_ observables on single particles. This framework has many connections to quantum chaos and has been proven to hold in such chaotic systems, but remains unproven for generic quantum systems, hence the "hypothesis" in the name. The second connection involves studying the effects of a quantum environment on a quantum system and essentially ignoring the state of the environment. By making several assumptions, such as an infinitely large environment, weak coupling, and an assumption known as Markovianity (where the system and environment remain in product states after each interaction), one can show that the thermal state is the unique fixed point for generic open systems. This is done using the Davies' Generators @davies1974markovian that give rise to a Linbladian evolution on the system with the desired fixed point. However, there exist many scenarios in nature where these assumptions are not met, such as finite sized environments that retain some correlations with the system and strong coupling situations. Finding Linblad evolutions that allow for generic thermalization for arbitrary system and environment pairs remains an open question. However, we do note that a flurry of quantum algorithms have been developed in recent years that are modeled off of these Linblad dynamics @chen2023quantumthermalstatepreparation @gilyen2024quantumgeneralizationsglaubermetropolis @ding2024efficientquantumgibbssamplers @cubitt2023dissipativegroundstatepreparation.

// The model for thermalization that we propose is instead rooted in the concepts developed by Hamiltonian Monte Carlo: prepare an additional register in the desired thermal state, let time evolution mix the two constituents, and then refresh the controllable register. As mentioned before, we will make do with just a single qubit ancilla. In order for this process to work there are two difficulties that must be overcome in the quantum mechanical setting: the first is that a Hamiltonian must be chosen for the single ancilla, which amounts to choosing an energy gap we denote $gamma$, and the second is that an interaction term must be chosen between the ancilla and the system. One of the main contributions of this thesis is a rigorous proof that choosing from a suitably random ensemble of interactions is sufficient for thermalization.

// This model turns out to have many beneficial properties that existing methods do not have. The first is that this model is very straightforward to implement on a quantum computer using existing algorithms. Evolution by a time independent Hamiltonian is the most complicated subroutine necessary. We will develop these primitives from scratch, showing how simple product formulas can be implemented at the gate level by utilizing a Pauli decomposition of the Hamiltonian. @ch:composite_simulations provides a new framework for developing new product formulas from existing ones. These ideas were developed in @hagan2022composite and allow for one to combine deterministic and randomized product formulas into a single channel. This makes it particularly well suited to simulating environmental effects on a system.

// One other property that our model for thermalization has is that it explicitly models the environment, keeping track of it's non-equilibrium state throughout the evolution. This allows for us to turn the problem of preparing the system in a thermal state to instead allow for our ancilla qubit to probe and extract information about the system. For example, if we know that the system is already in a thermal state we can allow it the thermalize our ancilla qubit. By measuring the temperature of the ancilla we can infer the temperature of the system, allowing us to do thermometry without creating an interaction model. This is advantageous for complicated quantum systems, as there currently only exists thermometry techniques for harmonic oscillators.

// The remainder of this thesis is organized as follows. In @ch:composite_simulations we introduce techniques for implementing existing product formulas on quantum computers, along with introducing new techniques for composing multiple simulation techniques for partitioned Hamiltonians. In @ch:thermal_state_prep we demonstrate how these techniques can be used to prepare thermal quantum states on fault-tolerant quantum computers. These two chapters represent the main technical contributions of this thesis and draw on the papers @hagan2022composite and @hagan2025thermodynamic for the analytic ideas and @pocrnic2024composite for numerical studies.


#heading("Introduction", level: 1, supplement: [Chapter]) <ch:intro>
A central goal of physics is to provide explanatory power for natural phenomenon observed in the laboratory. As we progress through the $21^"st"$ century, which at the time of writing is a quarter complete, the ability of our mathematics and current computers to provide succint and human understandable explanations is struggling with the sheer scale of complexity for many problems involving quantum mechanics. Simulating a large molecule with around 25 or so electrons is intractable for even the largest exascale ($10^15$ FLOPS) supercomputers available. Even if a supercomputer to simulate 25 electrons could be built, simulating 26 electrons would require one twice as large.

This exponential increase in the difficulty of simulating quantum mechanical systems, exponential with respect to the number of particles, led Manin @manin1980 and Feynman @feynman2018simulating in the early 1980s to propose using computers based on quantum mechanics to simulate quantum systems. The first concrete algorithm for doing so was proposed in 1996 by Lloyd @lloyd1996universal.

Temperature is probably the most common phenomenon that is discussed by scientists and non-scientists alike. Vaguely having something to do with energy, we know that placing hot objects and cold objects together leads to something in the middle, given enough time. This intuitive understanding is rooted scientifically in the concept of thermal equilibrium, a fairly well understood phenomenon for macroscopic classical systems. There do exist phenomenon, such as solitons, that do not equilibrate but most systems in contact with their environment tend to approach thermal equilibrium with it. The three main mechanisms for equilibration are pretty straightforward as well: radiation describes how photons can be emitted from any black-body and interact with target particles, conduction is the process of kinetic vibrations of structures leading to the transmition of energy between objects in contact with each other, and convection describes how liquids can transfer heat through currents.

These classical interactions are completely insufficient for describing thermalization of quantum systems. One of the primary points of tension in developing a quantum theory of thermodynamics is that thermodynamics is concerned with very large scales, typically an infinite number of particles and an infinite time. Quantum mechanics, on the other hand, was developed to explain the behavior of sub-atomic particles. Solving quantum mechanical systems is typically intractable for more than one or two particles, unless a lot of symmetry is present in the system that greatly constrains the possible solutions present (such as in condensed matter physics). It is difficult to describe quantum mechanically a system interacting with a macroscopic environment and there does not yet exist a comprehensive theory of quantum thermal equilibrium.

This difficulty poses a problem for simulating quantum systems on quantum computers, as we typically need to prepare thermal states of quantum systems as inputs to simulations. The most common scenario is to measure system energies via phase estimation, but measuring observables, such as magnetization or conductivity, at different temperatures is routinely done by physicists. This means when developing quantum algorithms to create these input states is much trickier than the classical scenario. General purpose thermal state preparation algorithms inspired by physical theories have only been developed in the past few years. The main contribution of this thesis is to not only provide a provably correct quantum algorithm for preparing thermal states on a quantum computer, but also to extend one of the three main theories proposed for quantum thermalization.

There are three main approaches to explain thermal equilibrium in quantum states. The earliest and most studied is more a collection of techniques utilizing Linbladian based dynamics. This approach essentially studies the net effect that an environment has on a quantum system. Through various approximations, such as weak-coupling between system and environment and a memoryless environment, one can come up with interactions that lead to thermal equilibrium of the system. However, these assumptions are not universal and understanding how to extend the study of open quantum systems into areas such as Markovian baths and strong coupling are active areas of research.

The second main theory is known as the Eigenstate Thermalization Hypothesis (ETH). This theory is designed to study how a large _closed_ quantum system can have subsytems that appear thermal. In effect, the system acts as it's own bath. This contradicts our intuition of closed quantum systems, as we typically think of unitary dynamics as conserving energy and therefore unable to explore a canonical ensemble. The secret to ETH's success is that the act of looking only at a small subsystem for sufficiently "chaotic" Hamiltonians can cause entanglement between the subsystems to create a reduced density matrix that appears thermal. Although ETH has shown to hold for quantum systems that resemble chaotic classical systems it remains to be proven for arbitrary Hamiltonians and the existence of counterexamples, known as many-body localization, is a heavily debated area.

The third theory of thermalization for quantum systems is the Repeated Interactions (RI) framework. In this model a simple environment, such as a single photon, is prepared in a thermal state and allowed to interact with the system. After the interaction, the environment is traced out and the process repeated. This has been shown to lead to thermal states for small systems, such as a single qubit or a three level system, and with fixed interactions. This is the model that we extend in this thesis.

The main technique we introduce to extend the RI framework to more general systems is to utilize a random interaction between the system and a single qubit environment. Our choice of ensemble for this random interaction, which is just a Pauli $Z$ string that is conjugated by a Haar random unitary, allows for transitions between energy levels of the system whenever the energy gap of the environment is tuned close enough to the system difference. For a system like a truncated harmonic oscillator this means we only have to find the fundamental gap of the oscillator. For a generic system the problem is much harder. One of our main results, @thm_zero_knowledge, is that one can prepare an approximate fixed point of the dynamics by choosing the environment gap uniformly at random from an interval containing all of the eigenvalue differences of the system. This robustness is both useful algorithmically, as we don't have to know the eigenvalues of the system to prepare it's thermal state, and important physically. In regards to physics, this tells us that if Nature works fundamentally via a RI phenomenon then it can do so blindly without being precise about the energy gaps of various systems.

== Existing Quantum Algorithms


=== Hamiltonian Simulation
Hamiltonian simulation is a `BQP`-Complete problem, meaning that any polynomial time quantum algorithm can be formulated as an instance of Hamiltonian simulation. It is one of the earliest quantum algorithms @lloyd1996universal and was the original inspiration for proposing quantum computers @feynman2018simulating. As such there has been a plethora of techniques for solving this problem and we cannot attempt a thorough review of the literature. We will instead mention at a high level the various techniques applicable to the results in this thesis.

To start, we will assume that the Hamiltonian $H$ is provided as a sum of Pauli operators $H = sum_(i = 1)^L h_i P_i$. This is not a theoretically restricted model, as all matrices can be written as a linear sum of Pauli operators, but it may not be a practical input model for all scenarios. For simulating fermions this can be a viable option due to mappings, such as the Jordan-Wigner and Bravyi-Kitaev encodings @Setia_2018, from fermionic operators to Pauli operators. There are two other main input models used, one in which $H$ is provided as a sparse matrix @aharonov2003adiabatic and the entries can be queried via an oracle and the other in which the matrix $H$ is encoded as a block of a unitary matrix on a larger Hilbert space @low2019hamiltonian. These input models are necessary for simulation algorithms with asymptotically optimal circuit depth @low2019hamiltonian but one downside is they require a logarithmic overhead in the number of qubits needed to perform the simulation. Pauli operators on the other hand can be can be exponentiated in the form $e^(i P theta)$ with no ancilla overhead, Figure 4.19 of Nielsen and Chuang @nielsen2002quantum, making product formulas memory optimal. Further, they do not take advantage of the commutator structure of the terms in the Hamiltonian, meaning Hamiltonians with a closed Lie algebraic structure can have provably better peformance @childs2021theory.



=== Thermal State Preparation

There are many existing quantum algorithms for preparing thermal states on quantum computers, this section will present those most related to our work. The first algorithm was developed by Poulin and Wocjan @poulin2009sampling and was based around using quantum phase estimation to work in the system's eigenbasis. By obtaining an entangled register of the eigenvector and eigenvalue $ket(psi_i)ket(E_i)$ one can then perform a Pauli $X$ rotation of an ancilla based on the contents of the energy register $E_i$. Using amplitude amplification on this ancilla then prepares the final state, which can be used to estimate thermal observables. This algorithm is robust and fairly straightforward to implement. The downsides are that it doubles the amount of qubits needed to represent the system and takes at least time $O(sqrt(dim / cal(Z)))$, which scales with the dimension in the ground state limit. This runtime is fairly rigid and there are not many avenues for special case analysis that may lead to improvements.

Efforts to adopt the Metropolis-Hastings algorithm, a workhorse routine for Monte Carlo techniques, began a little later. The first efforts by Temme @temme2011 work by straightforwardly implementing the Metropolis-Hastings algorithm, which we will not discuss here but can be found in @betancourt2018conceptualintroductionhamiltonianmonte, and solving the problem of rejecting samples via Marriot-Watrous rewinding @marriott2005quantum. This technique is rather delicate, and as a result the proof of correctness of this algorithm is typically seen as incomplete. Other methods have attempted to utilize techniques such as Szegedy walks @yung2012quantum but implicitly require infinite precision phase estimation, a computationally unrealistic task. Recently these effects have been mitigated @jiang2024quantummetropolissamplingweak, but the resulting algorithm still involves the use of phase estimation, a memory intensive operation.

As preparing thermal states is a `QMA`-Hard problem, we do not expect quantum algorithms to work efficiently for every possible system. The only way to avoid this general case difficulty is to focus on specific systems, commonly lattice systems as they are common in condensed matter and high energy physics. In @brandao_finite_2019 the decay of correlations in a lattice Hamiltonian is taken advantage of to provide thermal state preparation algorithms that scale sublinearly in the dimension of the Hilbert space. Similar ideas have been extended in the Quantum Imaginary Time Evolution (QITE) algorithm @motta2020determining, which approximates imaginary time evolution, or application of the operator $prop e^(- beta H)$, with local unitary operators. These algorithms fundamentally rely on notions of locality and therefore do not work for arbitrary systems.

Other techniques tend to be less inspired by phyics and more with traditional computer science techniques. For example, variational approaches @chowdhury2020variationalquantumalgorithmpreparing have strong inspirations from optimization algorithms. With the development of modern quantum algorithmic primitives, such as block-encodings, Quantum Signal Processing @low2017optimal, Linear Combinations of Unitaries (LCU) @childs2012hamiltonian, and Qauntum Singular Value Transformations (QSVT) @gilyen2019quantum, came along their application to thermal state preparation @chowdhury2016quantumalgorithmsgibbssampling @gilyen2019quantum. As the naive application of QSVT-like techniques to implement $e^(-beta H)$ tend to have circuit depths scaling as $O(e^(beta))$, due to amplitude amplification costs, these techniques are typically used as subroutines in other algorithms.

The last class of algorithms we discuss are inspired by dissipation with an environment. The most straightforward of these algorithms is the Dissipative Quantum Eigensolver @cubitt2023dissipativegroundstatepreparation, which prepares ground states via weak measurements. This algorithm is a stopped process, meaning it's runtime is a random variable and must be analyzed in expectation. This algorithm relies on weak measurements and as a result can be implemented with a single ancilla qubit, making it memory optimal. Extensions @zhang2023dissipative later showed how to tweak these weak measurements to prepare finite $beta$ Gibbs states.

The largest group of the dissipative algorithms are those based on simulating Linbladian operators @gilyen2024quantumgeneralizationsglaubermetropolis @zhan2025rapidquantumgroundstate @chen2023quantumthermalstatepreparation. The first of these algorithms was inspired by the Davies generators @davies1974markovian for open quantum sytems, which weakly couples a system to a infinite thermal bath. These algorithms construct a Linbladian that can be shown to satisfy fixed point conditions, typically the Kubo-Martin-Schwinger (KMS) conditions @kms1 @kms2, that lead to the thermal state of the system being fixed. The runtime of the algorithm then depends on bounding a quantity known as the "mixing time", which is a very active area of research @tong2025fastmixingweaklyinteracting @temme2013mixing @ramkumar2024mixingtimequantumgibbs. As Linbladians, also called Liouvillians due to the Liouvillian equation, are the primary technical tool for studying open quantum systems these algorithms have many decades of physical intuition to rely on when they will perform well. One last difficulty for quantum algorithms developers in utilizing these theories is to create unitaries that can be run on quantum computers that replicate the physical maps. There currently exist a variety of Linblad solvers @pocrnic2024quantumsimulationlindbladiandynamics @ding2024simulating @chen2023quantumthermalstatepreparation but each introduces a nontrivial overhead making an end-to-end analysis difficult.

The closest algorithm for thermal state preparation to those that we present in this thesis is a repeated interaction algorithm by Shtanko and Movassagh @shtanko2023preparingthermalstatesnoiseless. This algorithm works by preparing many qubits in a thermal state for randomly chosen gaps and then interacting these qubits via $k$-local Pauli strings. By simulating the time evolution of the system, along with these interactions, for a randomly chosen time and coupling constant, they are able to show rapid convergence to the thermal state for ETH satisfying Hamiltonians. Their algorithm for generic Hamiltonians does not work for all error tolerances and inverse temperatures $beta$. In @shtanko2023preparingthermalstatesnoiseless they mention that algorithms working for arbitrary low temperatures (high $beta$) and generic Hamiltonians was an open problem at their time of writing, which we have resolved.

== Organization of the thesis
This thesis aims to develop techniques that are capable of preparing quantum thermal states starting with only basic quantum computing primitives. In @ch:composite_simulations we show how arbitrary single qubit rotations and Controlled `NOT` gates can be utilized to implement a class of Hamiltonian simulations techniques known as product formulas. Hamiltonian simulation is a fundamental primitive in virtually all thermal state preparation algorithms, but particularly in ours. The product formulas we present in this chapter rely on the Hamiltonian being expressed as a sum of Pauli operators, which is always theoretically possible but may not be practical for all systems. This chapter introduces two existing product formulas, Trotter-Suzuki or just Trotter formulas which are deterministic and QDrift, which is a randomized product formula. We then extend this to a composite scenario, introduced in @hagan2023composite, where both Trotter and QDrift are used to build a partially randomized product formula. In @sec:composite_first_order we present the composition of a first order Trotter formula with a QDrift channel utilizing a first order decomposition into the two channels. In @sec:composite_higher_order we extend these results to higher order product formulas. We then present numerics in @sec_composite_numerics from the extension of these ideas to imaginary time evolution by Pocrnic et al. in @pocrnic2024composite. These empirical results confirm that the theoretic cost savings predicted theoretically can be realized in practice, even for small systems that can be simulated classically.

We then utilize these results in @ch:thermal_state_prep to realize our thermal state preparation algorithm. We first develop a weak-coupling expansion of our thermalizing channel in @sec_tsp_weak_coupling. One of the central tools we develop in this section is a reduction of our quantum dynamics to a classical Markov chain. This allows us to compute fixed points and determine how many applications of the quantum channel are necessary to reach a given trace distance $epsilon$ from the fixed point. In @sec_tsp_oscillator we apply these techniques to two specific systems: a two-level or qubit system and a more general truncated harmonic oscillator. We keep these two systems as separate as they tend to be of independent interest to researchers and our presentation avoids the more complicated environment gap averaging necessary for arbitrary systems. In @sec_tsp_generic_sys we provide thermalization results for arbitrary systems in two scenarios: one in which eigenvalue differences are known exactly and the other in which only a bound on the spectral norm $norm(H)$ is known.
