#import "macros.typ": *
#import "conf.typ": *

#heading("Conclusion", level: 1, supplement: "Chapter") <ch_conclusion>

After 30+ years of intense theoretic development, the most advanced end-to-end pipeline we have for utilizing potential quantum computers beyond the capabilities of classical computers is the simulation of quantum systems. Factoring large integers via Shor's algorithm is one provable advantage, assuming widely adopted complexity theoretic conjectures such as `BQP` $!=$ `BPP`. However, the advancing maturity @alagic2024status of lattice-based cryptosystems implies that using quantum computers solely for factoring integers has decreasing utility as the time to reach thousands of logical qubits grows. Many other provable end-to-end quantum speedups, such as Grover's algorithm for unstructured search or Brandao's Semi-Definite Program solver, only offer quadratic improvements over classical techniques. Evidence suggests that these speedups would have to be applied to unfeasibly large instance sizes in order for these speedups to provide an advange over classical computers.

Recent efforts have also explored larger polynomial improvements, such as the quadratic improvement for planted noisy $k$XOR problem @schmidhuber2024quarticquantumspeedupsplanted, but progress remains sparse. There exist other exponential speedups for problems such as Glued Trees traversal or Sunflower graph traversal, but these problems are relatively contrived and have yet to find many "real world" applications. The most difficult to analyze speedups are those that rely only on existing classical algorithms, such as normalized Betti number estimation for clique homology @berry2024analyzing, optimization, and linear systems solving. This is due to the lack of good classical lower bounds, and sometimes the same traits that make a problem amenable to fast quantum algorithms can lead to better classical algorithms. More recently, efforts have been made to probe the approximation ratios provided by quantum computers on difficult combinatorial optimization problems @jordan2024optimization, which also suffers the same problem that benchmarks can only be made against existing classical algorithms.

Simulating quantum systems has the advantage of being one of the oldest quantum algorithms @lloyd1996universal and as a result has seen much more intense scrutiny. Further, the problem of estimating observables of quantum states has been studied for many decades longer @metropolis1953equation and we have a decent understanding of when classical methods tend to fail. This understanding is constantly changing @lee2023evaluating with the advent of new classical algorithms, such as Density Matrix Renormalization Group (DMRG) methods @white1992density and tensor networks @orus2019tensor, but many papers @EQAQC expect early quantum computers to be useful for the electronic structure problem in medium sized molecules with strong electron correlation. Condensed matter systems and models such as the Fermi-Hubbard model @kan2024resource @kivlichan2020improved are some of the most promising candidates for quantum advantage. This is due to their inherent symmetries, which makes for easy to analyze quantum algorithms, and the strong correlation between electrons makes classical algorithms difficult to scale @leblanc2015solutions.

In this thesis, we presented two new quantum algorithms that can be added to the quantum simulation toolkit. The first is an extension of product formulas to include both random and deterministic sections. We were able to provide generic conditions on when these composite simulations can provide advantages over their constituent methods. We find that systems where the strength of the Hamiltonian terms decays exponentially, i.e. if $H = sum_i H_i$ then $norm(H_i) prop 2^(-i)$, have a relatively large parameter window for improvement. These analytically derived cost advanteges were then verified numerically for small systems. Further, this algorithm was extended to imaginary time evolution and provides new avenues for improving classical estimations of quantum observables.

The second contribution we make is an algorithm for preparing thermal states $e^(-beta H) / tr(e^(-beta H))$ on a quantum computer. This algorithm assumes access to the Hamiltonian only via time evolution operators $e^(i H t)$ and adds only a single ancilla qubit. At a high level, our technique works by randomizing over all possible interaction terms that could be present between this ancilla qubit and the system of interest. By randomly choosing the energy gap of the Hamiltonian dictating the time evolution of the ancilla qubit we can rigorously show that the thermal state is an approximate fixed point at finite inverse temperature $beta$ if no knowledge of the Hamiltonian spectrum is assumed. We can show that the thermal state is the unique fixed point exactly in the ground state limit ($beta -> oo$) or if the eigenvalue differences of $H$ are known. Remarkably, we are able to bound the number of interactions, the coupling strength, and the time per interaction needed in order to thermalize the system state to within trace distance $tilde(O)(epsilon)$ of the thermal state.

Taken as a whole, this thesis provides a blueprint for studying thermal equilibrium for quantum states on digital fault tolerant quantum computers. The two main contributions we presented work very well together: our thermal state preparation algorithm reduces the problem of thermalization to quantum simulation with a deterministic system hamiltonian and a randomly chosen interaction. Our composite simulation algorithm analyzes this situation precisely and provides detailed error estimates and oracle query costs. This gives a direct method for compiling quantum circuits for estimating thermal observables using simple primitives that are likely to be highly optimized for quantum computers.

For the rest of this conclusion I would like to discuss possible extensions to these methods. First with our composite simulation algorithm. The most obvious extension we could make is to propose better partitioning schemes that can take advantage of commutativity between subsections of the Hamiltonian. A classic example of this is that when simulating the most basic Fermi-Hubbard models the on-site potentials are typically simulated for all sites and then the tunnelling terms are simulated. Ideally, one would like to be able to collect some level of commutator data, such as all pairs $norm([H_i, H_j])$ or even higher levels, and use this data along with spectral norm data to construct a better partition than the straightforward `chop`. This may incorporate more stages than just a single partition into $A + B$.

To extend these ideas further, one could incorporate different simulation techniques than product formulas into a composite simulation. In @hagan2023composite we proposed including block-encoding techniques, such as multiproduct formulas @low2019well or qubitization @low2019hamiltonian. It remains an open question if block-encoding constants, the dominant cost contributor for these techniques, can be significantly reduced by taking terms out of the block-encoding partition and into a product formula partition. The final possible improvement one could make to partitioning schemes is to include information about entanglement. This was recently shown to improve the analysis for Trotter formulas @zhao2024entanglement and extending these ideas to inform partitions for composite simulations is a natural extension.

In regards to our thermal state preparation routine there are a few avenues for improvement, which is needed as our provable scaling of $dim^16$ is prohibitively high for a naive implementation. To eliminate the dimensionful factors contributing to the cost there are three areas that need to be addressed. The first is that the Haar integration over the entire system and environment can be reduced based on symmetries of the system. For example, similar algorithms have been studied for specific systems with less general interaction terms @metcalf2021quantummarkovchainmonte @Young_2012 and have seen empirical success. The stringent requirements on the randomness of the interaction could be reduced if knowledge of the symmetries of the system are known. For example on 1D and 2D lattice models that are translationally invariant we could require that our interaction also be translationally invariant. This would make our interaction scale invariant with respect to the number of lattice sites and eliminate one of the factors of dimension, and efficient mixing times have been shown for Linbladian based thermal state preparation routines in this translationally invariant setting @zhan2025rapidquantumgroundstate.

The second factor of dimension that could be reduced is the introduction of $dim$ from the Markov relaxation theorem we used from @jerison2013general. This result is general purpose and works for an arbitrary non-reversable Markov chain, but the cost of this flexibility is a high overhead in the number of interactions needed. This could be reduced using knowledge of the distance of the input state to the fixed point, as is typical in many ground state preparation routines @Lin_2020.

The last factor of dimension that would need to be eliminated is from the remainder bound $norm(R_Phi)_1$. The factor of dimension appears as we only study thermalization with respect to trace distance from the thermal state. This is the most rigorous distance metric one could use, but may not be the most physically relevant metric. If one has a collection of observables $O_i$, then measuring the deviation $max_i abs(angle.l O_i angle.r_beta - tr(O_i Phi^(compose L) (rho)))$ is a more physically relevant error metric. Bounding a relaxed error metric for our thermalization routine for a physically relevant class of observables, such as two-body correlators for example, is a promising avenue of research that could lead to increased performance.

These three areas constitute the main introductions of dimensionful factors into the runtime analysis of our thermalization routine. One last area for potential improvements we would like to mention is the issue of strong-coupling analysis. Numerically, we observed significantly faster thermalization with strong-coupling at the penalty of a higher noise floor. This leads us to conjecture that significant savings could be had with a "cooling schedule", where the thermalization procedure begins with very strong coupling for short times that gradually gets lowered to small coupling constants and long per-interaction simulation times. Unfortunately our Taylor series expansion does not seem amenable to higher order analysis, and this must be studied numerically. Developing a theoretic understanding of strong-coupling could lead to much better algorithmic guarantees beyond numeric evidence. The previously mentioned avenue of studying specific observable distributions could be a problem better suited for studying a strong-coupling approach.

Lastly, we return to the problem laid out in @ch:intro of thermal equilibrium. As our channel works for arbitrary Hamiltonians at finite $beta$, this can effectively be viewed as a way for _defining_ thermal equilibrium. One way to do so would be to define the fixed point for the joint system-environment dynamics and declare that thermal equilibrium is whatever the fixed point is of this map. One logical way to view this is to take the "environment", which was a single qubit in our work, and instead view it as a probe. By then viewing the system as a reservoir, we can interact the probe with the system and use it's state to _define_ what it's temperature is. This gives a simple way of performing interaction agnostic thermometry.

Previous thermal state preparation algorithms fall into one of two camps: they can be "computational" in nature and do not mimic any natural processes or do try to mimic a system-environment interaction (except for ETH inspired algorithms which simulate a large closed system). As the computational algorithms do not have explicit models for an environment they do not provide new perspectives on physical system-environment interactions. Algorithms more inspired by natural processes tend to use the Linbladian formalism, which explicitly does not simulate the state of the environment and instead captures its effects only on the system. Turning to algorithms that do explicitly model a system-environment, many other works have used similar ideas @metcalf2021quantummarkovchainmonte @Young_2012 @shtanko2023preparingthermalstatesnoiseless but tend to use limited interaction models and therefore are only provably correct for a limited subset of systems. What makes our algorithm the first of it's kind is that it provably works for any non-degenerate Hamiltonians at any temperature, even ground states.

